#!/usr/bin/env python3
"""
Florence2 å›¾åƒæè¿°ç”Ÿæˆè„šæœ¬ï¼ˆç²¾ç®€ç‰ˆï¼‰
ç‹¬ç«‹ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼Œä¸ä¾èµ– AnyLabeling GUI
ç²¾ç®€ç‰ˆï¼šç§»é™¤äº†æ€§èƒ½ç›‘æµ‹å’Œæ—¶é—´è®¡ç®—åŠŸèƒ½ï¼Œåªä¿ç•™æ ¸å¿ƒè¯­ä¹‰ç”ŸæˆåŠŸèƒ½
"""

import warnings
import sys
import argparse
import threading
import gc
import os
from pathlib import Path
from unittest.mock import patch
from typing import Union, Optional

warnings.filterwarnings("ignore")

# ç¿»è¯‘ç›¸å…³å¯¼å…¥
from transformers import MarianMTModel, MarianTokenizer

try:
    import torch
    from PIL import Image
    import numpy as np
    from transformers import AutoModelForCausalLM, AutoProcessor
    from transformers.dynamic_module_utils import get_imports
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow numpy")
    sys.exit(1)

# å¯¼å…¥ ROS2 ç›¸å…³åº“
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as ROSImage
from std_msgs.msg import Int8, String, Bool

# å¯¼å…¥ cv_bridge å’Œ OpenCVï¼Œç”¨äºå›¾åƒè½¬æ¢
from cv_bridge import CvBridge
import cv2


class Florence2Caption:
    """Florence2 å›¾åƒæè¿°ç”Ÿæˆå™¨ï¼ˆç²¾ç®€ç‰ˆï¼‰"""

    # ä»»åŠ¡ç±»å‹æ˜ å°„
    TASK_MAPPING = {
        "caption": "<CAPTION>",
        "detailed_cap": "<DETAILED_CAPTION>",
        "more_detailed_cap": "<MORE_DETAILED_CAPTION>",
    }

    def __init__(
        self,
        model_path: str,
        task_type: str = "caption",
        trust_remote_code: bool = True,
        max_new_tokens: int = 1024,
        do_sample: bool = False,
        num_beams: int = 3,
    ):
        """
        åˆå§‹åŒ– Florence2 æ¨¡å‹

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼‰
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰ "caption", "detailed_cap", "more_detailed_cap"
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: beam search çš„ beam æ•°é‡
        """
        if task_type not in self.TASK_MAPPING:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(self.TASK_MAPPING.keys())}"
            )

        self.task_type = task_type
        self.task_token = self.TASK_MAPPING[task_type]
        self.max_new_tokens = max_new_tokens
        self.do_sample = do_sample
        self.num_beams = num_beams

        # å±•å¼€è·¯å¾„ä¸­çš„ ~ ç¬¦å·ä¸ºç»å¯¹è·¯å¾„
        # HuggingFace åº“æ— æ³•è¯†åˆ«åŒ…å« ~ çš„è·¯å¾„ï¼Œéœ€è¦å±•å¼€
        if '~' in model_path:
            model_path = os.path.expanduser(model_path)
            # å±•å¼€åå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if not os.path.isabs(model_path):
                model_path = os.path.abspath(model_path)

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = (
            torch.float16 if torch.cuda.is_available() else torch.float32
        )

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ä¿®å¤ CPU ä¸Š flash_attn çš„é—®é¢˜
        def fixed_get_imports(filename):
            imports = get_imports(filename)
            if not torch.cuda.is_available() and "flash_attn" in imports:
                imports.remove("flash_attn")
            return imports

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        with patch(
            "transformers.dynamic_module_utils.get_imports", fixed_get_imports
        ):
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                device_map=self.device,
                trust_remote_code=trust_remote_code,
                attn_implementation="eager",
            )
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=trust_remote_code,
            )

        print(f"âœ… Caption æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç¿»è¯‘æ¨¡å‹ï¼ˆå¯é€‰ï¼ŒæŒ‰éœ€åŠ è½½ï¼‰
        self.translator = None
        self.translate_to_chinese = False

    def set_translation(
        self, 
        enable: bool = True, 
        model_name: str = "Helsinki-NLP/opus-mt-en-zh",
        model_path: Optional[str] = None
    ):
        """
        è®¾ç½®æ˜¯å¦å¯ç”¨ç¿»è¯‘åŠŸèƒ½
        
        Args:
            enable: æ˜¯å¦å¯ç”¨ç¿»è¯‘
            model_name: ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼‰ï¼Œå½“ model_path ä¸º None æ—¶ä½¿ç”¨
                      - "Helsinki-NLP/opus-mt-en-zh" (æ¨èï¼Œè‹±æ–‡åˆ°ä¸­æ–‡)
                      - "facebook/nllb-200-distilled-600M" (å¤šè¯­è¨€ï¼Œéœ€è¦æŒ‡å®šè¯­è¨€ä»£ç )
            model_path: æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
        """
        self.translate_to_chinese = enable
        
        if enable and self.translator is None:
            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
            if model_path:
                # å±•å¼€è·¯å¾„ä¸­çš„ ~ ç¬¦å·ä¸ºç»å¯¹è·¯å¾„
                if '~' in model_path:
                    model_path = os.path.expanduser(model_path)
                    # å±•å¼€åå¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    if not os.path.isabs(model_path):
                        model_path = os.path.abspath(model_path)
                
                if Path(model_path).exists():
                    print(f"ğŸ”„ æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_path}")
                    try:
                        self.translator_tokenizer = MarianTokenizer.from_pretrained(model_path)
                        self.translator_model = MarianMTModel.from_pretrained(model_path)
                        if torch.cuda.is_available():
                            self.translator_model = self.translator_model.to(self.device)
                        print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆæœ¬åœ°è·¯å¾„ï¼‰")
                    except Exception as e:
                        print(f"âš ï¸  ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹å¤±è´¥: {e}")
                        self.translate_to_chinese = False
                else:
                    print(f"âš ï¸  æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œå°†ä½¿ç”¨ HuggingFace æ¨¡å‹")
                    # å›é€€åˆ° HuggingFace æ¨¡å‹
                    model_path = None
            
            if enable and model_path is None:
                print(f"ğŸ”„ æ­£åœ¨ä» HuggingFace åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_name}")
                try:
                    self.translator_tokenizer = MarianTokenizer.from_pretrained(model_name)
                    self.translator_model = MarianMTModel.from_pretrained(model_name)
                    if torch.cuda.is_available():
                        self.translator_model = self.translator_model.to(self.device)
                    print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆHuggingFaceï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ç¿»è¯‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.translate_to_chinese = False

    def _translate_to_chinese(self, text: str) -> str:
        """
        å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡
        
        Args:
            text: è‹±æ–‡æ–‡æœ¬
            
        Returns:
            ä¸­æ–‡æ–‡æœ¬
        """
        if not self.translate_to_chinese or self.translator_model is None:
            return text
        
        try:
            # ç¿»è¯‘
            inputs = self.translator_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                translated = self.translator_model.generate(**inputs, max_length=512)
            
            translated_text = self.translator_tokenizer.decode(translated[0], skip_special_tokens=True)
            return translated_text
        except Exception as e:
            print(f"âš ï¸  ç¿»è¯‘å¤±è´¥: {e}ï¼Œè¿”å›åŸæ–‡")
            return text

    def generate_caption(
        self, 
        image: Union[str, Image.Image, np.ndarray]
    ) -> str:
        """
        ä¸ºå›¾åƒç”Ÿæˆæè¿°

        Args:
            image: å›¾åƒè¾“å…¥ï¼Œå¯ä»¥æ˜¯ï¼š
                  - str: å›¾åƒæ–‡ä»¶è·¯å¾„
                  - PIL.Image: PIL å›¾åƒå¯¹è±¡
                  - np.ndarray: numpy æ•°ç»„ï¼ˆRGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼‰

        Returns:
            å›¾åƒæè¿°æ–‡æœ¬
        """
        # è¯»å–å’Œè½¬æ¢å›¾åƒ
        if isinstance(image, str):
            # æ–‡ä»¶è·¯å¾„
            if not Path(image).exists():
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}")
            pil_image = Image.open(image).convert("RGB")
        elif isinstance(image, Image.Image):
            # PIL.Image
            pil_image = image.convert("RGB")
        elif isinstance(image, np.ndarray):
            # numpy æ•°ç»„
            if len(image.shape) != 3 or image.shape[2] != 3:
                raise ValueError(f"numpy æ•°ç»„å¿…é¡»æ˜¯ RGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼Œå½“å‰: {image.shape}")
            pil_image = Image.fromarray(image.astype(np.uint8))
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}ï¼Œæ”¯æŒç±»å‹: str, PIL.Image, np.ndarray")

        # é¢„å¤„ç†
        prompt = self.task_token
        inputs = self.processor(
            text=prompt, images=pil_image, return_tensors="pt"
        )

        # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡å¹¶åŒ¹é…æ¨¡å‹æ•°æ®ç±»å‹
        model_dtype = next(self.model.parameters()).dtype
        inputs = {
            k: (
                v.to(device=self.device, dtype=model_dtype)
                if torch.is_floating_point(v)
                else v.to(self.device)
            )
            for k, v in inputs.items()
        }

        # ç”Ÿæˆæè¿°
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆæè¿°...")
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=self.max_new_tokens,
                do_sample=self.do_sample,
                num_beams=self.num_beams,
                use_cache=False,  # ç¦ç”¨ç¼“å­˜ä»¥é¿å… past_key_values ä¸º None çš„é—®é¢˜
            )

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.processor.batch_decode(
            generated_ids, skip_special_tokens=False
        )[0]

        # åå¤„ç†è·å–æè¿°
        results = self.processor.post_process_generation(
            generated_text, task=self.task_token, image_size=pil_image.size
        )

        # æå–æè¿°æ–‡æœ¬
        if self.task_token in results:
            caption = results[self.task_token]
            if isinstance(caption, str):
                final_caption = caption
            elif isinstance(caption, dict) and "caption" in caption:
                final_caption = caption["caption"]
            else:
                final_caption = str(caption)
        else:
            final_caption = generated_text

        # å¦‚æœå¯ç”¨äº†ç¿»è¯‘ï¼Œå°†è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡
        if self.translate_to_chinese:
            print("ğŸ”„ æ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
            final_caption = self._translate_to_chinese(final_caption)

        return final_caption

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "processor"):
            del self.processor
        if hasattr(self, "translator_model"):
            del self.translator_model
        if hasattr(self, "translator_tokenizer"):
            del self.translator_tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# ROS2 è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰
class Florence2ControlNode(Node):
    """
    è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
    - æŒç»­æ¥æ”¶å›¾åƒæµï¼Œä¿å­˜æœ€æ–°ä¸€å¸§
    - ç›‘å¬æ§åˆ¶ä¿¡å·
    - æ”¶åˆ°ä¿¡å· 1 æ—¶ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹ã€å¤„ç†å›¾åƒã€é‡Šæ”¾èµ„æº
    """
    
    def __init__(self):
        super().__init__('florence2_control_node')
        
        # å‚æ•°å£°æ˜
        self.declare_parameter('image_source', 'ros2')  # å›¾åƒæ¥æº: "ros2" æˆ– "rtsp"
        self.declare_parameter('image_topic', '/camera/camera/color/image_raw')  # ROS2 å›¾åƒè¯é¢˜
        self.declare_parameter('rtsp_url', 'rtsp://192.168.168.168:8554/test')  # RTSP æµåœ°å€
        self.declare_parameter('control_topic', '/navigation/florence')  # æ§åˆ¶ä¿¡å·è¯é¢˜ 1 (Stringç±»å‹ï¼Œè§¦å‘è¯: "æ“åœº")
        self.declare_parameter('control_topic_2', '/nav/arrival')  # æ§åˆ¶ä¿¡å·è¯é¢˜ 2 (Int8ç±»å‹ï¼ŒæœŸæœ›å€¼: 1ï¼Œè§¦å‘å‘é€)
        self.declare_parameter('model_path', '/home/ubun/xanylabeling_data/models/florence')
        self.declare_parameter('task_type', 'more_detailed_cap')
        self.declare_parameter('result_topic', '/florence2/caption')
        self.declare_parameter('max_new_tokens', 1024)
        self.declare_parameter('num_beams', 3)
        self.declare_parameter('do_sample', False)
        self.declare_parameter('trust_remote_code', True)
        self.declare_parameter('translate_to_chinese', True)  # æ˜¯å¦ç¿»è¯‘ä¸ºä¸­æ–‡
        self.declare_parameter('translation_model', 'Helsinki-NLP/opus-mt-en-zh')  # ç¿»è¯‘æ¨¡å‹ï¼ˆHuggingFace IDï¼‰
        self.declare_parameter('translation_model_path', '')  # ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        self.declare_parameter('flip', False)  # æ˜¯å¦åœ¨è¯­ä¹‰ç”Ÿæˆå‰å°†å›¾åƒæ—‹è½¬180åº¦
        
        # è·å–å›¾åƒæºç±»å‹
        image_source = self.get_parameter('image_source').value
        
        # çº¿ç¨‹å®‰å…¨ï¼šæœ€æ–°å›¾åƒå­˜å‚¨
        self.latest_image_lock = threading.Lock()
        self.latest_image_msg = None  # ROS2 å›¾åƒæ¶ˆæ¯
        self.latest_rtsp_frame = None  # RTSP å¸§ï¼ˆnumpy arrayï¼‰
        
        # å¤„ç†çŠ¶æ€æ ‡å¿—ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        self.is_processing = False
        self.processing_lock = threading.Lock()
        
        # ç»“æœç¼“å­˜
        self.cached_result = None
        self.cache_lock = threading.Lock()
        
        # RTSP ç›¸å…³
        self.rtsp_cap = None
        self.rtsp_thread = None
        self.rtsp_running = False
        
        # åˆå§‹åŒ– cv_bridgeï¼ˆç”¨äºå›¾åƒè½¬æ¢ï¼‰
        self.cv_bridge = CvBridge()
        
        # æ ¹æ®å›¾åƒæºç±»å‹åˆå§‹åŒ–
        if image_source == 'ros2':
            # ROS2 æ¨¡å¼ï¼šåˆ›å»ºå›¾åƒè®¢é˜…è€…
            image_topic = self.get_parameter('image_topic').value
            self.image_subscription = self.create_subscription(
                ROSImage,
                image_topic,
                self.image_callback,
                1  # QoS depth = 1ï¼Œåªä¿ç•™æœ€æ–°å›¾åƒ
            )
            self.get_logger().info(f'ğŸ“· å·²è®¢é˜…å›¾åƒè¯é¢˜: {image_topic}')
        elif image_source == 'rtsp':
            # RTSP æ¨¡å¼ï¼šå¯åŠ¨ RTSP æµè¯»å–çº¿ç¨‹
            rtsp_url = self.get_parameter('rtsp_url').value
            self._start_rtsp_stream(rtsp_url)
        else:
            raise ValueError(f'ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}ï¼Œæ”¯æŒçš„ç±»å‹: "ros2", "rtsp"')
        
        # åˆ›å»ºæ§åˆ¶ä¿¡å·è®¢é˜…è€…ï¼ˆString ç±»å‹ï¼Œæ¥æ”¶ "æ“åœº" ç­‰è§¦å‘è¯ï¼‰
        # è®¢é˜…ç¬¬ä¸€ä¸ªæ§åˆ¶è¯é¢˜
        control_topic = self.get_parameter('control_topic').value
        self.control_subscription = self.create_subscription(
            String,
            control_topic,
            self.control_callback,
            10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
        )
        self.get_logger().info(f'ğŸ® å·²è®¢é˜…æ§åˆ¶ä¿¡å·è¯é¢˜ 1: {control_topic} (Stringç±»å‹ï¼Œè§¦å‘è¯: "æ“åœº")')
        
        # è®¢é˜…ç¬¬äºŒä¸ªæ§åˆ¶è¯é¢˜ï¼ˆå¦‚æœé…ç½®äº†ä¸”ä¸è¯é¢˜1ä¸åŒï¼‰
        control_topic_2 = self.get_parameter('control_topic_2').value
        if control_topic_2 and control_topic_2 != control_topic:
            self.control_subscription_2 = self.create_subscription(
                Int8,
                control_topic_2,
                self.control_callback_2,
                10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
            )
            self.get_logger().info(f'ğŸ® å·²è®¢é˜…æ§åˆ¶ä¿¡å·è¯é¢˜ 2: {control_topic_2} (Int8ç±»å‹ï¼ŒæœŸæœ›å€¼: 1ï¼Œè§¦å‘å‘é€)')
        else:
            self.control_subscription_2 = None
            if control_topic_2 == control_topic:
                self.get_logger().warn(f'âš ï¸  æ§åˆ¶è¯é¢˜ 2 ä¸è¯é¢˜ 1 ç›¸åŒï¼Œè·³è¿‡é‡å¤è®¢é˜…')
        
        # åˆ›å»ºç»“æœå‘å¸ƒè€…
        result_topic = self.get_parameter('result_topic').value
        self.caption_publisher = self.create_publisher(
            String,
            result_topic,
            10
        )
        self.get_logger().info(f'ğŸ“¤ å·²åˆ›å»ºç»“æœå‘å¸ƒè¯é¢˜: {result_topic}')
        
        self.get_logger().info('âœ… Florence2 Control Node åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰')
        self.get_logger().info('â³ ç­‰å¾…æ§åˆ¶ä¿¡å·...')
    
    def image_callback(self, msg: ROSImage):
        """
        å›¾åƒè¯é¢˜å›è°ƒå‡½æ•° - æŒç»­æ¥æ”¶ï¼Œä¿å­˜æœ€æ–°ä¸€å¸§ï¼ˆROS2 æ¨¡å¼ï¼‰
        """
        with self.latest_image_lock:
            self.latest_image_msg = msg
    
    def _start_rtsp_stream(self, rtsp_url: str):
        """
        å¯åŠ¨ RTSP æµè¯»å–çº¿ç¨‹
        
        Args:
            rtsp_url: RTSP æµåœ°å€
        """
        self.get_logger().info(f'ğŸ”„ æ­£åœ¨è¿æ¥ RTSP æµ: {rtsp_url}')
        
        # åˆ›å»º VideoCapture å¯¹è±¡
        self.rtsp_cap = cv2.VideoCapture(rtsp_url)
        
        if not self.rtsp_cap.isOpened():
            self.get_logger().error(f'âŒ æ— æ³•æ‰“å¼€ RTSP æµ: {rtsp_url}')
            raise RuntimeError(f'æ— æ³•æ‰“å¼€ RTSP æµ: {rtsp_url}')
        
        # è®¾ç½®ç¼“å†²åŒºå¤§å°ï¼ˆå‡å°‘å»¶è¿Ÿï¼‰
        self.rtsp_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        self.rtsp_running = True
        
        # å¯åŠ¨è¯»å–çº¿ç¨‹
        self.rtsp_thread = threading.Thread(target=self._rtsp_read_loop, daemon=True)
        self.rtsp_thread.start()
        
        self.get_logger().info(f'âœ… RTSP æµè¯»å–çº¿ç¨‹å·²å¯åŠ¨: {rtsp_url}')
    
    def _rtsp_read_loop(self):
        """
        RTSP æµè¯»å–å¾ªç¯ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        """
        while self.rtsp_running:
            ret, frame = self.rtsp_cap.read()
            if ret:
                # è½¬æ¢ä¸º RGB æ ¼å¼
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                with self.latest_image_lock:
                    self.latest_rtsp_frame = frame_rgb
            else:
                self.get_logger().warn('âš ï¸  RTSP æµè¯»å–å¤±è´¥ï¼Œå°è¯•é‡æ–°è¿æ¥...')
                # å°è¯•é‡æ–°è¿æ¥
                self.rtsp_cap.release()
                import time
                time.sleep(1)
                rtsp_url = self.get_parameter('rtsp_url').value
                self.rtsp_cap = cv2.VideoCapture(rtsp_url)
                if not self.rtsp_cap.isOpened():
                    self.get_logger().error(f'âŒ RTSP æµé‡è¿å¤±è´¥: {rtsp_url}')
                    break
                self.rtsp_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        # æ¸…ç†èµ„æº
        if self.rtsp_cap is not None:
            self.rtsp_cap.release()
            self.get_logger().info('ğŸ”„ RTSP æµå·²å…³é—­')
    
    def control_callback(self, msg: String):
        """
        æ§åˆ¶ä¿¡å·å›è°ƒå‡½æ•° 1
        msg.data: å½“æ¥æ”¶åˆ° "æ“åœº" æ—¶ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹è¿›è¡Œè§£æï¼Œä½†ä¸å‘é€ç»“æœï¼ˆåªç¼“å­˜ï¼‰
        """
        trigger_word = msg.data.strip()
        
        if trigger_word != "æ“åœº":
            # ä¸æ˜¯è§¦å‘è¯ï¼Œè·³è¿‡å¤„ç†
            self.get_logger().debug(f'æ”¶åˆ°æ§åˆ¶ä¿¡å·: "{trigger_word}"ï¼Œä¸æ˜¯è§¦å‘è¯ "æ“åœº"ï¼Œè·³è¿‡å¤„ç†')
            return
        
        # æ”¶åˆ° "æ“åœº"ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹å¹¶å¤„ç†ï¼ˆåªç¼“å­˜ï¼Œä¸å‘é€ï¼‰
        self.get_logger().info('æ”¶åˆ°æ§åˆ¶ä¿¡å· "æ“åœº": å¼€å§‹å¤„ç†å›¾åƒï¼ˆè§£æåç¼“å­˜ï¼Œç­‰å¾… control_topic_2 å‘é€ï¼‰...')
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        with self.processing_lock:
            if self.is_processing:
                self.get_logger().warn('ä¸Šä¸€æ¬¡å¤„ç†å°šæœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚')
                return
            self.is_processing = True
        
        try:
            # è·å–æœ€æ–°å›¾åƒï¼ˆæ ¹æ®å›¾åƒæºç±»å‹ï¼‰
            image_source = self.get_parameter('image_source').value
            
            if image_source == 'ros2':
                # ROS2 æ¨¡å¼ï¼šä»è¯é¢˜è·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_image_msg is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ°å›¾åƒï¼Œæ— æ³•å¤„ç†')
                        return
                    image_msg = self.latest_image_msg
                self._process_with_model(image_msg, send_result=False)
            elif image_source == 'rtsp':
                # RTSP æ¨¡å¼ï¼šä» RTSP æµè·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_rtsp_frame is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ° RTSP å¸§ï¼Œæ— æ³•å¤„ç†')
                        return
                    frame = self.latest_rtsp_frame.copy()
                self._process_with_rtsp_frame(frame, send_result=False)
            else:
                self.get_logger().error(f'âŒ ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}')
                return
            
        except Exception as e:
            self.get_logger().error(f'âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}')
            import traceback
            self.get_logger().error(traceback.format_exc())
        finally:
            with self.processing_lock:
                self.is_processing = False
    
    def control_callback_2(self, msg: Int8):
        """
        æ§åˆ¶ä¿¡å·å›è°ƒå‡½æ•° 2ï¼ˆInt8 ç±»å‹ï¼‰
        msg.data: å½“æ¥æ”¶åˆ° 1 æ—¶ï¼š
        - å¦‚æœæœ‰ç¼“å­˜ç»“æœï¼Œç›´æ¥å‘é€ç¼“å­˜ç»“æœï¼ˆä¸è¿›è¡Œè§£æï¼‰
        - å¦‚æœæ²¡æœ‰ç¼“å­˜ç»“æœï¼Œå¼€å§‹åŠ è½½æ¨¡å‹è¿›è¡Œè§£æå¹¶åœ¨è§£æå®Œæˆåå‘é€ç»“æœ
        """
        signal = msg.data
        
        if signal != 1:
            # ä¸æ˜¯æœŸæœ›å€¼ï¼Œè·³è¿‡å¤„ç†
            self.get_logger().debug(f'æ”¶åˆ°æ§åˆ¶ä¿¡å·: {signal}ï¼Œä¸æ˜¯æœŸæœ›å€¼ 1ï¼Œè·³è¿‡å¤„ç†')
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ç»“æœ
        with self.cache_lock:
            if self.cached_result is not None:
                # æœ‰ç¼“å­˜ç»“æœï¼Œç›´æ¥å‘é€ï¼Œä¸è¿›è¡Œè§£æ
                self.get_logger().info('ğŸ“¤ æ”¶åˆ°æ§åˆ¶ä¿¡å· 1: æ£€æµ‹åˆ°ç¼“å­˜ç»“æœï¼Œç›´æ¥å‘é€ï¼ˆè·³è¿‡è§£æï¼‰')
                cached = self.cached_result
                self.cached_result = None  # æ¸…ç©ºç¼“å­˜ï¼Œé¿å…é‡å¤ä½¿ç”¨
                self._publish_caption(cached)
                print("\033[36m" + "â”€" * 80 + "\033[0m")
                return
        
        # æ²¡æœ‰ç¼“å­˜ç»“æœï¼Œå¼€å§‹è§£æå¹¶åœ¨è§£æå®Œæˆåå‘é€
        self.get_logger().info('æ”¶åˆ°æ§åˆ¶ä¿¡å· 1: å¼€å§‹å¤„ç†å›¾åƒï¼ˆè§£æå®Œæˆåç«‹å³å‘é€ï¼‰...')
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        with self.processing_lock:
            if self.is_processing:
                self.get_logger().warn('ä¸Šä¸€æ¬¡å¤„ç†å°šæœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚')
                return
            self.is_processing = True
        
        try:
            # è·å–æœ€æ–°å›¾åƒï¼ˆæ ¹æ®å›¾åƒæºç±»å‹ï¼‰
            image_source = self.get_parameter('image_source').value
            
            if image_source == 'ros2':
                # ROS2 æ¨¡å¼ï¼šä»è¯é¢˜è·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_image_msg is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ°å›¾åƒï¼Œæ— æ³•å¤„ç†')
                        return
                    image_msg = self.latest_image_msg
                self._process_with_model(image_msg, send_result=True)
            elif image_source == 'rtsp':
                # RTSP æ¨¡å¼ï¼šä» RTSP æµè·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_rtsp_frame is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ° RTSP å¸§ï¼Œæ— æ³•å¤„ç†')
                        return
                    frame = self.latest_rtsp_frame.copy()
                self._process_with_rtsp_frame(frame, send_result=True)
            else:
                self.get_logger().error(f'âŒ ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}')
                return
            
        except Exception as e:
            self.get_logger().error(f'âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}')
            import traceback
            self.get_logger().error(traceback.format_exc())
        finally:
            with self.processing_lock:
                self.is_processing = False
    
    def _process_with_model(self, image_msg: ROSImage, send_result: bool = True):
        """
        æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¤„ç† ROS2 å›¾åƒæ¶ˆæ¯ï¼Œç„¶åé‡Šæ”¾èµ„æº
        
        Args:
            image_msg: ROS2 Image æ¶ˆæ¯
            send_result: æ˜¯å¦åœ¨è§£æå®Œæˆåç«‹å³å‘é€ç»“æœï¼ˆTrue=ç«‹å³å‘é€ï¼ŒFalse=åªç¼“å­˜ï¼‰
        """
        # 1. è½¬æ¢å›¾åƒ
        self.get_logger().info('ğŸ”„ è½¬æ¢å›¾åƒ...')
        pil_image = self._ros_image_to_pil(image_msg)
        self._process_image(pil_image, send_result=send_result)
    
    def _process_with_rtsp_frame(self, frame: np.ndarray, send_result: bool = True):
        """
        æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¤„ç† RTSP å¸§ï¼Œç„¶åé‡Šæ”¾èµ„æº
        
        Args:
            frame: RTSP å¸§ï¼ˆRGB numpy arrayï¼‰
            send_result: æ˜¯å¦åœ¨è§£æå®Œæˆåç«‹å³å‘é€ç»“æœï¼ˆTrue=ç«‹å³å‘é€ï¼ŒFalse=åªç¼“å­˜ï¼‰
        """
        # 1. è½¬æ¢å›¾åƒ
        self.get_logger().info('ğŸ”„ è½¬æ¢å›¾åƒ...')
        pil_image = Image.fromarray(frame)
        self._process_image(pil_image, send_result=send_result)
    
    def _process_image(self, pil_image: Image.Image, send_result: bool = True):
        """
        å¤„ç†å›¾åƒï¼ˆé€šç”¨æ–¹æ³•ï¼Œæ”¯æŒ ROS2 å’Œ RTSPï¼‰
        
        Args:
            pil_image: PIL Image å¯¹è±¡
            send_result: æ˜¯å¦åœ¨è§£æå®Œæˆåç«‹å³å‘é€ç»“æœï¼ˆTrue=ç«‹å³å‘é€ï¼ŒFalse=åªç¼“å­˜ï¼‰
        """
        caption_generator = None
        try:
            # 1.1 æ ¹æ® flip å‚æ•°å†³å®šæ˜¯å¦ç¿»è½¬å›¾åƒ
            flip = self.get_parameter('flip').value
            if flip:
                self.get_logger().info('ğŸ”„ æ­£åœ¨å°†å›¾åƒæ—‹è½¬180åº¦...')
                pil_image = pil_image.rotate(180)
            
            # 2. åŠ è½½æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
            self.get_logger().info('ğŸ”„ æ­£åœ¨åŠ è½½ Florence2 æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰...')
            caption_generator = Florence2Caption(
                model_path=self.get_parameter('model_path').value,
                task_type=self.get_parameter('task_type').value,
                max_new_tokens=self.get_parameter('max_new_tokens').value,
                num_beams=self.get_parameter('num_beams').value,
                do_sample=self.get_parameter('do_sample').value,
                trust_remote_code=self.get_parameter('trust_remote_code').value,
            )
            
            # 2.1 å¦‚æœå¯ç”¨ç¿»è¯‘ï¼ŒåŠ è½½ç¿»è¯‘æ¨¡å‹
            translate_to_chinese = self.get_parameter('translate_to_chinese').value
            if translate_to_chinese:
                translation_model = self.get_parameter('translation_model').value
                translation_model_path = self.get_parameter('translation_model_path').value
                # å¦‚æœè·¯å¾„ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨ Noneï¼ˆè¡¨ç¤ºä½¿ç”¨ HuggingFace æ¨¡å‹ï¼‰
                if translation_model_path == '':
                    translation_model_path = None
                caption_generator.set_translation(
                    enable=True, 
                    model_name=translation_model,
                    model_path=translation_model_path
                )
            
            # 3. ç”Ÿæˆæè¿°
            caption = caption_generator.generate_caption(pil_image)
            
            self.get_logger().info(f'âœ… ç”Ÿæˆæè¿°: {caption}')
            
            # 4. æ ¹æ® send_result å‚æ•°å†³å®šæ˜¯å‘é€è¿˜æ˜¯ç¼“å­˜
            with self.cache_lock:
                if send_result:
                    # ç«‹å³å‘é€ç»“æœ
                    self.get_logger().info('ğŸ“¤ è§£æå®Œæˆï¼Œç«‹å³å‘é€ç»“æœ')
                    self._publish_caption(caption)
                    # å‘é€åæ¸…ç©ºç¼“å­˜ï¼ˆç¡®ä¿ä¸ä¼šé‡å¤ä½¿ç”¨ï¼‰
                    self.cached_result = None
                else:
                    # åªç¼“å­˜ç»“æœï¼Œä¸å‘é€
                    self.get_logger().info('â³ è§£æå®Œæˆï¼Œç¼“å­˜ç»“æœï¼Œç­‰å¾… control_topic_2 ä¿¡å·å‘é€...')
                    self.cached_result = caption
            
        finally:
            # 5. é‡Šæ”¾æ¨¡å‹èµ„æº
            if caption_generator is not None:
                self.get_logger().info('ğŸ”„ æ­£åœ¨é‡Šæ”¾æ¨¡å‹èµ„æº...')
                self._cleanup_model(caption_generator)
                self.get_logger().info('âœ… æ¨¡å‹èµ„æºå·²é‡Šæ”¾')
                print("\033[36m" + "â”€" * 80 + "\033[0m")
    
    def _ros_image_to_pil(self, msg: ROSImage) -> Image.Image:
        """
        å°† ROS2 sensor_msgs/Image è½¬æ¢ä¸º PIL.Image
        
        Args:
            msg: ROS2 Image æ¶ˆæ¯
            
        Returns:
            PIL.Image å¯¹è±¡ï¼ˆRGB æ ¼å¼ï¼‰
        """
        try:
            # è½¬æ¢ä¸º OpenCV æ ¼å¼ (BGR)
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            # è½¬æ¢ä¸º RGB
            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸º PIL.Image
            pil_image = Image.fromarray(cv_image_rgb)
            return pil_image
        except Exception as e:
            self.get_logger().error(f'âŒ å›¾åƒè½¬æ¢å¤±è´¥: {e}')
            raise
    
    def _publish_caption(self, caption: str):
        """
        å‘å¸ƒæè¿°ç»“æœ
        
        Args:
            caption: å›¾åƒæè¿°æ–‡æœ¬
        """
        msg = String()
        msg.data = caption
        self.caption_publisher.publish(msg)
        self.get_logger().debug(f'ğŸ“¤ å·²å‘å¸ƒæè¿°ç»“æœ')
    
    def _cleanup_model(self, caption_generator: Florence2Caption):
        """
        æ¸…ç†æ¨¡å‹èµ„æº
        
        Args:
            caption_generator: Florence2Caption å®ä¾‹
        """
        try:
            # åˆ é™¤æ¨¡å‹å’Œå¤„ç†å™¨
            if hasattr(caption_generator, 'model'):
                del caption_generator.model
            if hasattr(caption_generator, 'processor'):
                del caption_generator.processor
            # åˆ é™¤ç¿»è¯‘æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(caption_generator, 'translator_model'):
                del caption_generator.translator_model
            if hasattr(caption_generator, 'translator_tokenizer'):
                del caption_generator.translator_tokenizer
            
            # æ¸…ç† GPU ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception as e:
            self.get_logger().warn(f'âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}')
    
    def destroy_node(self):
        """
        èŠ‚ç‚¹é”€æ¯æ—¶æ¸…ç†èµ„æº
        """
        # åœæ­¢ RTSP æµè¯»å–
        if self.rtsp_running:
            self.rtsp_running = False
            if self.rtsp_thread is not None:
                self.rtsp_thread.join(timeout=2.0)
            if self.rtsp_cap is not None:
                self.rtsp_cap.release()
        
        super().destroy_node()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼ˆç²¾ç®€ç‰ˆï¼Œæ”¯æŒå‘½ä»¤è¡Œå’Œ ROS2 æ¨¡å¼ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å‘½ä»¤è¡Œæ¨¡å¼
  python florence2_caption_ros2_lite.py --image path/to/image.jpg --model_path /path/to/model --task_type detailed_cap

  # ROS2 æ¨¡å¼
  python florence2_caption_ros2_lite.py --ros2
        """,
    )

    parser.add_argument(
        "--ros2",
        action="store_true",
        help="ä»¥ ROS2 èŠ‚ç‚¹æ¨¡å¼è¿è¡Œï¼ˆéœ€è¦ ROS2 ç¯å¢ƒï¼‰",
    )
    parser.add_argument(
        "--image",
        type=str,
        help="è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆå‘½ä»¤è¡Œæ¨¡å¼å¿…éœ€ï¼‰",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="/home/ubun/xanylabeling_data/models/florence",
        help="æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼Œå¦‚ microsoft/Florence-2-large-ftï¼‰",
    )
    parser.add_argument(
        "--task_type",
        type=str,
        default="more_detailed_cap",
        choices=["caption", "detailed_cap", "more_detailed_cap"],
        help="ä»»åŠ¡ç±»å‹: caption (åŸºç¡€æè¿°), detailed_cap (è¯¦ç»†æè¿°), more_detailed_cap (æ›´è¯¦ç»†æè¿°)",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=1024,
        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=3,
        help="Beam search çš„ beam æ•°é‡ï¼ˆé»˜è®¤: 3ï¼‰",
    )
    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="ä½¿ç”¨é‡‡æ ·ç”Ÿæˆï¼ˆé»˜è®¤: Falseï¼Œä½¿ç”¨ beam searchï¼‰",
    )
    parser.add_argument(
        "--translate_to_chinese",
        action="store_true",
        help="å°†ç”Ÿæˆçš„è‹±æ–‡æè¿°ç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆéœ€è¦é¢å¤–åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼‰",
    )
    parser.add_argument(
        "--translation_model",
        type=str,
        default="Helsinki-NLP/opus-mt-en-zh",
        help="ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼Œé»˜è®¤: Helsinki-NLP/opus-mt-en-zhï¼‰",
    )
    parser.add_argument(
        "--translation_model_path",
        type=str,
        default=None,
        help="ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„è€Œä¸æ˜¯ HuggingFace æ¨¡å‹ï¼‰",
    )

    # ä½¿ç”¨ parse_known_args ä»¥å…¼å®¹ ROS2 çš„ --ros-args / --params-file ç­‰å‚æ•°
    args, _ = parser.parse_known_args()

    # ROS2 æ¨¡å¼
    if args.ros2:
        try:
            rclpy.init()
            node = Florence2ControlNode()
            rclpy.spin(node)
        except KeyboardInterrupt:
            print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­èŠ‚ç‚¹...")
        except Exception as e:
            print(f"âŒ ROS2 èŠ‚ç‚¹è¿è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if rclpy.ok():
                rclpy.shutdown()
        return

    # å‘½ä»¤è¡Œæ¨¡å¼
    if not args.image:
        parser.error("å‘½ä»¤è¡Œæ¨¡å¼éœ€è¦ --image å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --ros2 è¿›å…¥ ROS2 æ¨¡å¼")

    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        caption_generator = Florence2Caption(
            model_path=args.model_path,
            task_type=args.task_type,
            max_new_tokens=args.max_new_tokens,
            do_sample=args.do_sample,
            num_beams=args.num_beams,
        )
        
        # å¦‚æœå¯ç”¨ç¿»è¯‘ï¼Œè®¾ç½®ç¿»è¯‘åŠŸèƒ½
        if args.translate_to_chinese:
            caption_generator.set_translation(
                enable=True, 
                model_name=args.translation_model,
                model_path=args.translation_model_path
            )

        # ç”Ÿæˆæè¿°
        caption = caption_generator.generate_caption(args.image)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“ å›¾åƒæè¿°:")
        print("=" * 60)
        print(caption)
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

