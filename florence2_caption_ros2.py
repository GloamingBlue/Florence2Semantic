#!/usr/bin/env python3
"""
Florence2 å›¾åƒæè¿°ç”Ÿæˆè„šæœ¬
ç‹¬ç«‹ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼Œä¸ä¾èµ– AnyLabeling GUI
"""

import warnings
import sys
import argparse
import time
import threading
from pathlib import Path
from unittest.mock import patch
from typing import Dict, Tuple, Union, Optional

warnings.filterwarnings("ignore")

try:
    import torch
    from PIL import Image
    import numpy as np
    from transformers import AutoModelForCausalLM, AutoProcessor
    from transformers.dynamic_module_utils import get_imports
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow numpy")
    sys.exit(1)

# å¯¼å…¥ ROS2 ç›¸å…³åº“
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as ROSImage
from std_msgs.msg import Int8, String, Bool

# å¯¼å…¥ cv_bridge å’Œ OpenCVï¼Œç”¨äºå›¾åƒè½¬æ¢
from cv_bridge import CvBridge
import cv2

# å¯¼å…¥ psutil ç”¨äºå†…å­˜ç›‘æ§
import psutil

# ç¿»è¯‘ç›¸å…³å¯¼å…¥
from transformers import MarianMTModel, MarianTokenizer


class Florence2Caption:
    """Florence2 å›¾åƒæè¿°ç”Ÿæˆå™¨"""

    # ä»»åŠ¡ç±»å‹æ˜ å°„
    TASK_MAPPING = {
        "caption": "<CAPTION>",
        "detailed_cap": "<DETAILED_CAPTION>",
        "more_detailed_cap": "<MORE_DETAILED_CAPTION>",
    }

    def __init__(
        self,
        model_path: str,
        task_type: str = "caption",
        trust_remote_code: bool = True,
        max_new_tokens: int = 1024,
        do_sample: bool = False,
        num_beams: int = 3,
    ):
        """
        åˆå§‹åŒ– Florence2 æ¨¡å‹

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼‰
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰ "caption", "detailed_cap", "more_detailed_cap"
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: beam search çš„ beam æ•°é‡
        """
        if task_type not in self.TASK_MAPPING:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(self.TASK_MAPPING.keys())}"
            )

        self.task_type = task_type
        self.task_token = self.TASK_MAPPING[task_type]
        self.max_new_tokens = max_new_tokens
        self.do_sample = do_sample
        self.num_beams = num_beams

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = (
            torch.float16 if torch.cuda.is_available() else torch.float32
        )

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ”¢ æ•°æ®ç±»å‹: {self.torch_dtype}")

        # æµ‹é‡æ¨¡å‹åŠ è½½æ—¶é—´
        load_start = time.perf_counter()

        # ä¿®å¤ CPU ä¸Š flash_attn çš„é—®é¢˜
        def fixed_get_imports(filename):
            imports = get_imports(filename)
            if not torch.cuda.is_available() and "flash_attn" in imports:
                imports.remove("flash_attn")
            return imports

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        with patch(
            "transformers.dynamic_module_utils.get_imports", fixed_get_imports
        ):
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                device_map=self.device,
                trust_remote_code=trust_remote_code,
                attn_implementation="eager",
            )
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=trust_remote_code,
            )

        load_end = time.perf_counter()
        self.load_time = load_end - load_start
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {self.load_time:.2f} ç§’)")

        # åˆå§‹åŒ– CUDA eventsï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
        self.use_cuda_events = torch.cuda.is_available()
        if self.use_cuda_events:
            self.start_event = torch.cuda.Event(enable_timing=True)
            self.end_event = torch.cuda.Event(enable_timing=True)
            # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()

        # è®°å½•æ¨¡å‹åŠ è½½åçš„å†…å­˜å’Œæ˜¾å­˜å ç”¨
        self.initial_memory = self._get_memory_usage()
        self.initial_gpu_memory = self._get_gpu_memory_usage()
        
        # ç¿»è¯‘æ¨¡å‹ï¼ˆå¯é€‰ï¼ŒæŒ‰éœ€åŠ è½½ï¼‰
        self.translator = None
        self.translate_to_chinese = False

    def _get_memory_usage(self) -> Dict[str, float]:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
        process = psutil.Process()
        mem_info = process.memory_info()
        return {
            "rss": mem_info.rss / (1024 * 1024),  # MB
            "vms": mem_info.vms / (1024 * 1024),  # MB
        }

    def _get_gpu_memory_usage(self) -> Union[Dict[str, float], None]:
        """
        è·å–å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰
        
        è¿”å›çš„æ˜¾å­˜ä¿¡æ¯è¯´æ˜ï¼š
        - allocated: å·²åˆ†é…æ˜¾å­˜ï¼ŒPyTorch å®é™…ç”¨äºå­˜å‚¨å¼ é‡æ•°æ®çš„æ˜¾å­˜
        - reserved: ä¿ç•™æ˜¾å­˜ï¼ŒPyTorch ä» CUDA åˆ†é…å™¨ä¿ç•™çš„æ€»æ˜¾å­˜
                   åŒ…æ‹¬å·²åˆ†é…çš„æ˜¾å­˜ + ç¼“å­˜æ± ä¸­çš„æ˜¾å­˜ï¼ˆç”¨äºå¿«é€Ÿåˆ†é…æ–°å¼ é‡ï¼‰
        - max_allocated: å³°å€¼å·²åˆ†é…æ˜¾å­˜ï¼Œè‡ªä¸Šæ¬¡ reset_peak_memory_stats() åçš„æœ€å¤§å€¼
        
        æ³¨æ„ï¼šreserved >= allocatedï¼Œå› ä¸º PyTorch ä¼šä¿ç•™ä¸€äº›æ˜¾å­˜ä½œä¸ºç¼“å­˜
        """
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
            reserved = torch.cuda.memory_reserved() / (1024 * 1024)  # MB
            max_allocated = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB
            return {
                "allocated": allocated,
                "reserved": reserved,
                "max_allocated": max_allocated,
            }
        return None

    def _format_bytes(self, bytes_value: float) -> str:
        """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»æ ¼å¼"""
        if bytes_value < 1024:
            return f"{bytes_value:.2f} MB"
        elif bytes_value < 1024 * 1024:
            return f"{bytes_value / 1024:.2f} GB"
        else:
            return f"{bytes_value / (1024 * 1024):.2f} TB"

    def generate_caption(
        self, 
        image: Union[str, Image.Image, np.ndarray], 
        return_timing: bool = False
    ) -> Union[str, Tuple[str, Dict[str, float]]]:
        """
        ä¸ºå›¾åƒç”Ÿæˆæè¿°

        Args:
            image: å›¾åƒè¾“å…¥ï¼Œå¯ä»¥æ˜¯ï¼š
                  - str: å›¾åƒæ–‡ä»¶è·¯å¾„
                  - PIL.Image: PIL å›¾åƒå¯¹è±¡
                  - np.ndarray: numpy æ•°ç»„ï¼ˆRGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼‰
            return_timing: æ˜¯å¦è¿”å›æ—¶é—´ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¦‚æœ return_timing=False: å›¾åƒæè¿°æ–‡æœ¬
            å¦‚æœ return_timing=True: (å›¾åƒæè¿°æ–‡æœ¬, æ—¶é—´ç»Ÿè®¡å­—å…¸)
        """
        timing_info = {}

        # è¯»å–å’Œè½¬æ¢å›¾åƒ
        read_start = time.perf_counter()
        
        if isinstance(image, str):
            # æ–‡ä»¶è·¯å¾„
            if not Path(image).exists():
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}")
            print(f"ğŸ“· æ­£åœ¨è¯»å–å›¾åƒ: {image}")
            pil_image = Image.open(image).convert("RGB")
        elif isinstance(image, Image.Image):
            # PIL.Image
            print(f"ğŸ“· ä½¿ç”¨æä¾›çš„ PIL å›¾åƒ")
            pil_image = image.convert("RGB")
        elif isinstance(image, np.ndarray):
            # numpy æ•°ç»„
            print(f"ğŸ“· ä½¿ç”¨æä¾›çš„ numpy æ•°ç»„å›¾åƒ")
            if len(image.shape) != 3 or image.shape[2] != 3:
                raise ValueError(f"numpy æ•°ç»„å¿…é¡»æ˜¯ RGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼Œå½“å‰: {image.shape}")
            pil_image = Image.fromarray(image.astype(np.uint8))
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}ï¼Œæ”¯æŒç±»å‹: str, PIL.Image, np.ndarray")
        
        read_end = time.perf_counter()
        timing_info["image_read"] = read_end - read_start

        # é¢„å¤„ç†
        preprocess_start = time.perf_counter()
        prompt = self.task_token
        print(f"ğŸ”¤ ä½¿ç”¨ä»»åŠ¡ç±»å‹: {self.task_type} ({self.task_token})")

        # è®°å½•æ¨ç†å‰çš„å†…å­˜å’Œæ˜¾å­˜
        memory_before = self._get_memory_usage()
        gpu_memory_before = self._get_gpu_memory_usage()
        
        # é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡
        if self.use_cuda_events:
            torch.cuda.reset_peak_memory_stats()

        inputs = self.processor(
            text=prompt, images=pil_image, return_tensors="pt"
        )

        # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡å¹¶åŒ¹é…æ¨¡å‹æ•°æ®ç±»å‹
        model_dtype = next(self.model.parameters()).dtype
        inputs = {
            k: (
                v.to(device=self.device, dtype=model_dtype)
                if torch.is_floating_point(v)
                else v.to(self.device)
            )
            for k, v in inputs.items()
        }

        # åŒæ­¥ GPUï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if self.use_cuda_events:
            torch.cuda.synchronize()

        preprocess_end = time.perf_counter()
        timing_info["preprocess"] = preprocess_end - preprocess_start

        # ç”Ÿæˆæè¿°
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆæè¿°...")
        
        # ä½¿ç”¨ CUDA events æµ‹é‡ GPU æ¨ç†æ—¶é—´ï¼ˆæ›´å‡†ç¡®ï¼‰
        if self.use_cuda_events:
            self.start_event.record()
        else:
            inference_start = time.perf_counter()

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=self.max_new_tokens,
                do_sample=self.do_sample,
                num_beams=self.num_beams,
                use_cache=False,  # ç¦ç”¨ç¼“å­˜ä»¥é¿å… past_key_values ä¸º None çš„é—®é¢˜
            )

        if self.use_cuda_events:
            self.end_event.record()
            torch.cuda.synchronize()
            timing_info["inference"] = (
                self.start_event.elapsed_time(self.end_event) / 1000.0
            )  # è½¬æ¢ä¸ºç§’
        else:
            inference_end = time.perf_counter()
            timing_info["inference"] = inference_end - inference_start

        # ç»Ÿè®¡ç”Ÿæˆçš„ token æ•°é‡
        num_generated_tokens = generated_ids.shape[1] - inputs["input_ids"].shape[1]
        timing_info["generated_tokens"] = num_generated_tokens
        if timing_info["inference"] > 0:
            timing_info["tokens_per_second"] = (
                num_generated_tokens / timing_info["inference"]
            )

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        decode_start = time.perf_counter()
        generated_text = self.processor.batch_decode(
            generated_ids, skip_special_tokens=False
        )[0]
        decode_end = time.perf_counter()
        timing_info["decode"] = decode_end - decode_start

        # åå¤„ç†è·å–æè¿°
        postprocess_start = time.perf_counter()
        results = self.processor.post_process_generation(
            generated_text, task=self.task_token, image_size=pil_image.size
        )

        # æå–æè¿°æ–‡æœ¬
        if self.task_token in results:
            caption = results[self.task_token]
            if isinstance(caption, str):
                final_caption = caption
            elif isinstance(caption, dict) and "caption" in caption:
                final_caption = caption["caption"]
            else:
                final_caption = str(caption)
        else:
            final_caption = generated_text

        postprocess_end = time.perf_counter()
        timing_info["postprocess"] = postprocess_end - postprocess_start

        # è®°å½•æ¨ç†åçš„å†…å­˜å’Œæ˜¾å­˜
        memory_after = self._get_memory_usage()
        gpu_memory_after = self._get_gpu_memory_usage()

        # è®¡ç®—å†…å­˜å’Œæ˜¾å­˜ä½¿ç”¨
        timing_info["memory"] = {
            "before": memory_before,
            "after": memory_after,
            "delta": {
                "rss": memory_after["rss"] - memory_before["rss"],
                "vms": memory_after["vms"] - memory_before["vms"],
            },
            "peak_rss": memory_after["rss"] - self.initial_memory["rss"],
        }

        if gpu_memory_before and gpu_memory_after:
            timing_info["gpu_memory"] = {
                "before": gpu_memory_before,
                "after": gpu_memory_after,
                "delta": {
                    "allocated": gpu_memory_after["allocated"]
                    - gpu_memory_before["allocated"],
                    "reserved": gpu_memory_after["reserved"]
                    - gpu_memory_before["reserved"],
                },
                "peak_allocated": gpu_memory_after["max_allocated"]
                - self.initial_gpu_memory["allocated"]
                if self.initial_gpu_memory
                else gpu_memory_after["max_allocated"],
            }
        else:
            timing_info["gpu_memory"] = None

        # å¦‚æœå¯ç”¨äº†ç¿»è¯‘ï¼Œå°†è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡
        if self.translate_to_chinese:
            translate_start = time.perf_counter()
            print("ğŸ”„ æ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
            final_caption = self._translate_to_chinese(final_caption)
            translate_end = time.perf_counter()
            timing_info["translation"] = translate_end - translate_start
        else:
            timing_info["translation"] = 0.0

        # è®¡ç®—æ€»æ—¶é—´
        timing_info["total"] = sum(
            [
                timing_info["image_read"],
                timing_info["preprocess"],
                timing_info["inference"],
                timing_info["decode"],
                timing_info["postprocess"],
                timing_info["translation"],
            ]
        )

        if return_timing:
            return final_caption, timing_info
        else:
            return final_caption

    def set_translation(
        self, 
        enable: bool = True, 
        model_name: str = "Helsinki-NLP/opus-mt-en-zh",
        model_path: Optional[str] = None
    ):
        """
        è®¾ç½®æ˜¯å¦å¯ç”¨ç¿»è¯‘åŠŸèƒ½
        
        Args:
            enable: æ˜¯å¦å¯ç”¨ç¿»è¯‘
            model_name: ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼‰ï¼Œå½“ model_path ä¸º None æ—¶ä½¿ç”¨
                      - "Helsinki-NLP/opus-mt-en-zh" (æ¨èï¼Œè‹±æ–‡åˆ°ä¸­æ–‡)
                      - "facebook/nllb-200-distilled-600M" (å¤šè¯­è¨€ï¼Œéœ€è¦æŒ‡å®šè¯­è¨€ä»£ç )
            model_path: æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
        """
        self.translate_to_chinese = enable
        
        if enable and self.translator is None:
            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
            if model_path and Path(model_path).exists():
                print(f"ğŸ”„ æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_path}")
                try:
                    self.translator_tokenizer = MarianTokenizer.from_pretrained(model_path)
                    self.translator_model = MarianMTModel.from_pretrained(model_path)
                    if torch.cuda.is_available():
                        self.translator_model = self.translator_model.to(self.device)
                    print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆæœ¬åœ°è·¯å¾„ï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹å¤±è´¥: {e}")
                    self.translate_to_chinese = False
            elif model_path:
                print(f"âš ï¸  æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œå°†ä½¿ç”¨ HuggingFace æ¨¡å‹")
                # å›é€€åˆ° HuggingFace æ¨¡å‹
                model_path = None
            
            if enable and model_path is None:
                print(f"ğŸ”„ æ­£åœ¨ä» HuggingFace åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_name}")
                try:
                    self.translator_tokenizer = MarianTokenizer.from_pretrained(model_name)
                    self.translator_model = MarianMTModel.from_pretrained(model_name)
                    if torch.cuda.is_available():
                        self.translator_model = self.translator_model.to(self.device)
                    print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆHuggingFaceï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ç¿»è¯‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.translate_to_chinese = False

    def _translate_to_chinese(self, text: str) -> str:
        """
        å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡
        
        Args:
            text: è‹±æ–‡æ–‡æœ¬
            
        Returns:
            ä¸­æ–‡æ–‡æœ¬
        """
        if not self.translate_to_chinese or self.translator_model is None:
            return text
        
        try:
            # ç¿»è¯‘
            inputs = self.translator_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                translated = self.translator_model.generate(**inputs, max_length=512)
            
            translated_text = self.translator_tokenizer.decode(translated[0], skip_special_tokens=True)
            return translated_text
        except Exception as e:
            print(f"âš ï¸  ç¿»è¯‘å¤±è´¥: {e}ï¼Œè¿”å›åŸæ–‡")
            return text

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "processor"):
            del self.processor
        if hasattr(self, "translator_model"):
            del self.translator_model
        if hasattr(self, "translator_tokenizer"):
            del self.translator_tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# ROS2 è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰
class Florence2ControlNode(Node):
    """
    è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
    - æŒç»­æ¥æ”¶å›¾åƒæµï¼Œä¿å­˜æœ€æ–°ä¸€å¸§
    - ç›‘å¬æ§åˆ¶ä¿¡å·
    - æ”¶åˆ°ä¿¡å· 1 æ—¶ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹ã€å¤„ç†å›¾åƒã€é‡Šæ”¾èµ„æº
    """
    
    def __init__(self):
        super().__init__('florence2_control_node')
        
        # å‚æ•°å£°æ˜
        self.declare_parameter('image_source', 'ros2')  # å›¾åƒæ¥æº: "ros2" æˆ– "rtsp"
        self.declare_parameter('image_topic', '/camera/camera/color/image_raw')  # ROS2 å›¾åƒè¯é¢˜
        self.declare_parameter('rtsp_url', 'rtsp://192.168.168.168:8554/test')  # RTSP æµåœ°å€
        self.declare_parameter('control_topic', '/navigation/florence')  # æ§åˆ¶ä¿¡å·è¯é¢˜ 1 (Stringç±»å‹ï¼Œè§¦å‘è¯: "æ“åœº")
        self.declare_parameter('control_topic_2', '/nav/arrival')  # æ§åˆ¶ä¿¡å·è¯é¢˜ 2 (Int8ç±»å‹ï¼ŒæœŸæœ›å€¼: 1ï¼Œå¯é€‰)
        self.declare_parameter('ready_topic', '/speech/ready')  # å‡†å¤‡æ¥å—ç»“æœçš„è¯é¢˜
        self.declare_parameter('model_path', '/home/ubun/xanylabeling_data/models/florence')
        self.declare_parameter('task_type', 'more_detailed_cap')
        self.declare_parameter('result_topic', '/florence2/caption')
        self.declare_parameter('show_timing', True)
        self.declare_parameter('max_new_tokens', 1024)
        self.declare_parameter('num_beams', 3)
        self.declare_parameter('do_sample', False)
        self.declare_parameter('trust_remote_code', True)
        self.declare_parameter('translate_to_chinese', False)  # æ˜¯å¦ç¿»è¯‘ä¸ºä¸­æ–‡
        self.declare_parameter('translation_model', 'Helsinki-NLP/opus-mt-en-zh')  # ç¿»è¯‘æ¨¡å‹ï¼ˆHuggingFace IDï¼‰
        self.declare_parameter('translation_model_path', '/home/ubun/xanylabeling_data/models/opus-mt-en-ch')  # ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        self.declare_parameter('flip', False)  # æ˜¯å¦åœ¨è¯­ä¹‰ç”Ÿæˆå‰å°†å›¾åƒæ—‹è½¬180åº¦
        
        # è·å–å›¾åƒæºç±»å‹
        image_source = self.get_parameter('image_source').value
        
        # çº¿ç¨‹å®‰å…¨ï¼šæœ€æ–°å›¾åƒå­˜å‚¨
        self.latest_image_lock = threading.Lock()
        self.latest_image_msg = None  # ROS2 å›¾åƒæ¶ˆæ¯
        self.latest_rtsp_frame = None  # RTSP å¸§ï¼ˆnumpy arrayï¼‰
        
        # å¤„ç†çŠ¶æ€æ ‡å¿—ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        self.is_processing = False
        self.processing_lock = threading.Lock()
        
        # Ready çŠ¶æ€å’Œç»“æœç¼“å­˜
        self.ready_received = False
        self.cached_result = None
        self.ready_lock = threading.Lock()
        
        # RTSP ç›¸å…³
        self.rtsp_cap = None
        self.rtsp_thread = None
        self.rtsp_running = False
        
        # åˆå§‹åŒ– cv_bridgeï¼ˆç”¨äºå›¾åƒè½¬æ¢ï¼‰
        self.cv_bridge = CvBridge()
        
        # æ ¹æ®å›¾åƒæºç±»å‹åˆå§‹åŒ–
        if image_source == 'ros2':
            # ROS2 æ¨¡å¼ï¼šåˆ›å»ºå›¾åƒè®¢é˜…è€…
            image_topic = self.get_parameter('image_topic').value
            self.image_subscription = self.create_subscription(
                ROSImage,
                image_topic,
                self.image_callback,
                1  # QoS depth = 1ï¼Œåªä¿ç•™æœ€æ–°å›¾åƒ
            )
            self.get_logger().info(f'ğŸ“· å·²è®¢é˜…å›¾åƒè¯é¢˜: {image_topic}')
        elif image_source == 'rtsp':
            # RTSP æ¨¡å¼ï¼šå¯åŠ¨ RTSP æµè¯»å–çº¿ç¨‹
            rtsp_url = self.get_parameter('rtsp_url').value
            self._start_rtsp_stream(rtsp_url)
        else:
            raise ValueError(f'ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}ï¼Œæ”¯æŒçš„ç±»å‹: "ros2", "rtsp"')
        
        # åˆ›å»ºæ§åˆ¶ä¿¡å·è®¢é˜…è€…ï¼ˆString ç±»å‹ï¼Œæ¥æ”¶ "æ“åœº" ç­‰è§¦å‘è¯ï¼‰
        # è®¢é˜…ç¬¬ä¸€ä¸ªæ§åˆ¶è¯é¢˜
        control_topic = self.get_parameter('control_topic').value
        self.control_subscription = self.create_subscription(
            String,
            control_topic,
            self.control_callback,
            10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
        )
        self.get_logger().info(f'ğŸ® å·²è®¢é˜…æ§åˆ¶ä¿¡å·è¯é¢˜ 1: {control_topic} (Stringç±»å‹ï¼Œè§¦å‘è¯: "æ“åœº")')
        
        # è®¢é˜…ç¬¬äºŒä¸ªæ§åˆ¶è¯é¢˜ï¼ˆå¦‚æœé…ç½®äº†ä¸”ä¸è¯é¢˜1ä¸åŒï¼‰
        control_topic_2 = self.get_parameter('control_topic_2').value
        if control_topic_2 and control_topic_2 != control_topic:
            self.control_subscription_2 = self.create_subscription(
                Int8,
                control_topic_2,
                self.control_callback_2,
                10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
            )
            self.get_logger().info(f'ğŸ® å·²è®¢é˜…æ§åˆ¶ä¿¡å·è¯é¢˜ 2: {control_topic_2} (Int8ç±»å‹ï¼ŒæœŸæœ›å€¼: 1)')
        else:
            self.control_subscription_2 = None
            if control_topic_2 == control_topic:
                self.get_logger().warn(f'âš ï¸  æ§åˆ¶è¯é¢˜ 2 ä¸è¯é¢˜ 1 ç›¸åŒï¼Œè·³è¿‡é‡å¤è®¢é˜…')
        
        # åˆ›å»º ready ä¿¡å·è®¢é˜…è€…
        ready_topic = self.get_parameter('ready_topic').value
        self.ready_subscription = self.create_subscription(
            Bool,
            ready_topic,
            self.ready_callback,
            10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
        )
        self.get_logger().info(f'âœ… å·²è®¢é˜…å‡†å¤‡ä¿¡å·è¯é¢˜: {ready_topic} (Boolç±»å‹)')
        
        # åˆ›å»ºç»“æœå‘å¸ƒè€…
        result_topic = self.get_parameter('result_topic').value
        self.caption_publisher = self.create_publisher(
            String,
            result_topic,
            10
        )
        self.get_logger().info(f'ğŸ“¤ å·²åˆ›å»ºç»“æœå‘å¸ƒè¯é¢˜: {result_topic}')

        self.get_logger().info('âœ… Florence2 Caption Node åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰')
        self.get_logger().info('â³ ç­‰å¾…æ§åˆ¶ä¿¡å·...')
    
    def image_callback(self, msg: ROSImage):
        """
        å›¾åƒè¯é¢˜å›è°ƒå‡½æ•° - æŒç»­æ¥æ”¶ï¼Œä¿å­˜æœ€æ–°ä¸€å¸§ï¼ˆROS2 æ¨¡å¼ï¼‰
        """
        with self.latest_image_lock:
            self.latest_image_msg = msg
        # ä¸ç«‹å³å¤„ç†ï¼Œç­‰å¾…æ§åˆ¶ä¿¡å·
    
    def _start_rtsp_stream(self, rtsp_url: str):
        """
        å¯åŠ¨ RTSP æµè¯»å–çº¿ç¨‹
        
        Args:
            rtsp_url: RTSP æµåœ°å€
        """
        self.get_logger().info(f'ğŸ”„ æ­£åœ¨è¿æ¥ RTSP æµ: {rtsp_url}')
        
        # åˆ›å»º VideoCapture å¯¹è±¡
        self.rtsp_cap = cv2.VideoCapture(rtsp_url)
        
        if not self.rtsp_cap.isOpened():
            self.get_logger().error(f'âŒ æ— æ³•æ‰“å¼€ RTSP æµ: {rtsp_url}')
            raise RuntimeError(f'æ— æ³•æ‰“å¼€ RTSP æµ: {rtsp_url}')
        
        # è®¾ç½®ç¼“å†²åŒºå¤§å°ï¼ˆå‡å°‘å»¶è¿Ÿï¼‰
        self.rtsp_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        # å°è¯•è¯»å–ç¬¬ä¸€å¸§ï¼ŒéªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸
        self.get_logger().info('ğŸ”„ æ­£åœ¨éªŒè¯ RTSP æµè¿æ¥...')
        ret, test_frame = self.rtsp_cap.read()
        if not ret or test_frame is None:
            self.rtsp_cap.release()
            self.get_logger().error(f'âŒ RTSP æµè¿æ¥éªŒè¯å¤±è´¥ï¼Œæ— æ³•è¯»å–ç¬¬ä¸€å¸§: {rtsp_url}')
            raise RuntimeError(f'RTSP æµè¿æ¥éªŒè¯å¤±è´¥: {rtsp_url}')
        
        self.get_logger().info(f'âœ… RTSP æµè¿æ¥éªŒè¯æˆåŠŸï¼Œå·²è¯»å–ç¬¬ä¸€å¸§ (å°ºå¯¸: {test_frame.shape})')
        
        self.rtsp_running = True
        
        # å¯åŠ¨è¯»å–çº¿ç¨‹
        self.rtsp_thread = threading.Thread(target=self._rtsp_read_loop, daemon=True)
        self.rtsp_thread.start()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿çº¿ç¨‹å·²å¯åŠ¨å¹¶å¼€å§‹è¯»å–
        time.sleep(0.5)
        
        self.get_logger().info(f'âœ… RTSP æµè¯»å–çº¿ç¨‹å·²å¯åŠ¨: {rtsp_url}')
    
    def _rtsp_read_loop(self):
        """
        RTSP æµè¯»å–å¾ªç¯ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        """
        frame_count = 0
        consecutive_failures = 0
        max_failures = 5
        
        while self.rtsp_running:
            ret, frame = self.rtsp_cap.read()
            if ret:
                # è½¬æ¢ä¸º RGB æ ¼å¼
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                with self.latest_image_lock:
                    self.latest_rtsp_frame = frame_rgb
                
                frame_count += 1
                consecutive_failures = 0
                
                # æ¯100å¸§è¾“å‡ºä¸€æ¬¡çŠ¶æ€ï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                if frame_count % 100 == 0:
                    self.get_logger().debug(f'ğŸ“¹ RTSP æµæ­£å¸¸ï¼Œå·²è¯»å– {frame_count} å¸§')
            else:
                consecutive_failures += 1
                self.get_logger().warn(f'âš ï¸  RTSP æµè¯»å–å¤±è´¥ (è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡)ï¼Œå°è¯•é‡æ–°è¿æ¥...')
                
                # å¦‚æœè¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå°è¯•é‡æ–°è¿æ¥
                if consecutive_failures >= max_failures:
                    # å°è¯•é‡æ–°è¿æ¥
                    self.rtsp_cap.release()
                    time.sleep(2)  # ç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¿
                    rtsp_url = self.get_parameter('rtsp_url').value
                    self.get_logger().info(f'ğŸ”„ æ­£åœ¨é‡æ–°è¿æ¥ RTSP æµ: {rtsp_url}')
                    self.rtsp_cap = cv2.VideoCapture(rtsp_url)
                    if not self.rtsp_cap.isOpened():
                        self.get_logger().error(f'âŒ RTSP æµé‡è¿å¤±è´¥: {rtsp_url}')
                        # æ¸…ç©ºå½“å‰å¸§
                        with self.latest_image_lock:
                            self.latest_rtsp_frame = None
                        break
                    self.rtsp_cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                    consecutive_failures = 0
                    self.get_logger().info('âœ… RTSP æµé‡è¿æˆåŠŸ')
        
        # æ¸…ç†èµ„æº
        if self.rtsp_cap is not None:
            self.rtsp_cap.release()
            self.get_logger().info('ğŸ”„ RTSP æµå·²å…³é—­')
    
    def control_callback(self, msg: String):
        """
        æ§åˆ¶ä¿¡å·å›è°ƒå‡½æ•°
        msg.data: å½“æ¥æ”¶åˆ° "æ“åœº" æ—¶ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹è¿›è¡Œè§£æ
        """
        trigger_word = msg.data.strip()
        
        if trigger_word != "æ“åœº":
            # ä¸æ˜¯è§¦å‘è¯ï¼Œè·³è¿‡å¤„ç†
            self.get_logger().debug(f'æ”¶åˆ°æ§åˆ¶ä¿¡å·: "{trigger_word}"ï¼Œä¸æ˜¯è§¦å‘è¯ "æ“åœº"ï¼Œè·³è¿‡å¤„ç†')
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ç»“æœï¼Œå¦‚æœæœ‰å°±ä¸å†æ¬¡è§£æ
        with self.ready_lock:
            if self.cached_result is not None:
                self.get_logger().info('âš ï¸  å·²æœ‰ç¼“å­˜ç»“æœï¼Œè·³è¿‡æœ¬æ¬¡è§£æè¯·æ±‚ï¼ˆç­‰å¾… ready ä¿¡å·å‘é€ï¼‰')
                return
        
        # æ”¶åˆ° "æ“åœº"ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹å¹¶å¤„ç†
        self.get_logger().info('æ”¶åˆ°æ§åˆ¶ä¿¡å· "æ“åœº": å¼€å§‹å¤„ç†å›¾åƒ...')
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        with self.processing_lock:
            if self.is_processing:
                self.get_logger().warn('ä¸Šä¸€æ¬¡å¤„ç†å°šæœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚')
                return
            self.is_processing = True
        
        try:
            # è·å–æœ€æ–°å›¾åƒï¼ˆæ ¹æ®å›¾åƒæºç±»å‹ï¼‰
            image_source = self.get_parameter('image_source').value
            
            if image_source == 'ros2':
                # ROS2 æ¨¡å¼ï¼šä»è¯é¢˜è·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_image_msg is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ°å›¾åƒï¼Œæ— æ³•å¤„ç†')
                        return
                    image_msg = self.latest_image_msg
                self._process_with_model(image_msg)
            elif image_source == 'rtsp':
                # RTSP æ¨¡å¼ï¼šä» RTSP æµè·å–å›¾åƒ
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ RTSP æµå·²ç»è¯»å–åˆ°å¸§
                max_wait_time = 3.0  # æœ€å¤šç­‰å¾…3ç§’
                wait_interval = 0.1  # æ¯æ¬¡æ£€æŸ¥é—´éš”0.1ç§’
                waited_time = 0.0
                
                while waited_time < max_wait_time:
                    with self.latest_image_lock:
                        if self.latest_rtsp_frame is not None:
                            frame = self.latest_rtsp_frame.copy()
                            break
                    time.sleep(wait_interval)
                    waited_time += wait_interval
                else:
                    # è¶…æ—¶ä»æœªæ”¶åˆ°å¸§
                    self.get_logger().warn(f'âš ï¸  ç­‰å¾… {max_wait_time} ç§’åä»æœªæ”¶åˆ° RTSP å¸§ï¼Œæ— æ³•å¤„ç†')
                    self.get_logger().warn('ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ RTSP æµåœ°å€æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸')
                    # æ£€æŸ¥ RTSP æµçŠ¶æ€
                    if self.rtsp_cap is None or not self.rtsp_cap.isOpened():
                        self.get_logger().error('âŒ RTSP æµè¿æ¥å·²æ–­å¼€')
                    if not self.rtsp_running:
                        self.get_logger().error('âŒ RTSP è¯»å–çº¿ç¨‹å·²åœæ­¢')
                    return
                
                self._process_with_rtsp_frame(frame)
            else:
                self.get_logger().error(f'âŒ ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}')
                return
            
        except Exception as e:
            self.get_logger().error(f'âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}')
            import traceback
            self.get_logger().error(traceback.format_exc())
        finally:
            with self.processing_lock:
                self.is_processing = False
    
    def control_callback_2(self, msg: Int8):
        """
        æ§åˆ¶ä¿¡å·å›è°ƒå‡½æ•° 2ï¼ˆInt8 ç±»å‹ï¼‰
        msg.data: å½“æ¥æ”¶åˆ° 1 æ—¶ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹è¿›è¡Œè§£æ
        """
        signal = msg.data
        
        if signal != 1:
            # ä¸æ˜¯æœŸæœ›å€¼ï¼Œè·³è¿‡å¤„ç†
            self.get_logger().debug(f'æ”¶åˆ°æ§åˆ¶ä¿¡å·: {signal}ï¼Œä¸æ˜¯æœŸæœ›å€¼ 1ï¼Œè·³è¿‡å¤„ç†')
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ç»“æœï¼Œå¦‚æœæœ‰å°±ä¸å†æ¬¡è§£æ
        with self.ready_lock:
            if self.cached_result is not None:
                self.get_logger().info('âš ï¸  å·²æœ‰ç¼“å­˜ç»“æœï¼Œè·³è¿‡æœ¬æ¬¡è§£æè¯·æ±‚ï¼ˆç­‰å¾… ready ä¿¡å·å‘é€ï¼‰')
                return
        
        # æ”¶åˆ° 1ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹å¹¶å¤„ç†
        self.get_logger().info('æ”¶åˆ°æ§åˆ¶ä¿¡å· 1: å¼€å§‹å¤„ç†å›¾åƒ...')
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        with self.processing_lock:
            if self.is_processing:
                self.get_logger().warn('ä¸Šä¸€æ¬¡å¤„ç†å°šæœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚')
                return
            self.is_processing = True
        
        try:
            # è·å–æœ€æ–°å›¾åƒï¼ˆæ ¹æ®å›¾åƒæºç±»å‹ï¼‰
            image_source = self.get_parameter('image_source').value
            
            if image_source == 'ros2':
                # ROS2 æ¨¡å¼ï¼šä»è¯é¢˜è·å–å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_image_msg is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ°å›¾åƒï¼Œæ— æ³•å¤„ç†')
                        return
                    image_msg = self.latest_image_msg
                self._process_with_model(image_msg)
            elif image_source == 'rtsp':
                # RTSP æ¨¡å¼ï¼šä» RTSP æµè·å–å›¾åƒ
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ RTSP æµå·²ç»è¯»å–åˆ°å¸§
                max_wait_time = 3.0  # æœ€å¤šç­‰å¾…3ç§’
                wait_interval = 0.1  # æ¯æ¬¡æ£€æŸ¥é—´éš”0.1ç§’
                waited_time = 0.0
                
                while waited_time < max_wait_time:
                    with self.latest_image_lock:
                        if self.latest_rtsp_frame is not None:
                            frame = self.latest_rtsp_frame.copy()
                            break
                    time.sleep(wait_interval)
                    waited_time += wait_interval
                else:
                    # è¶…æ—¶ä»æœªæ”¶åˆ°å¸§
                    self.get_logger().warn(f'âš ï¸  ç­‰å¾… {max_wait_time} ç§’åä»æœªæ”¶åˆ° RTSP å¸§ï¼Œæ— æ³•å¤„ç†')
                    self.get_logger().warn('ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ RTSP æµåœ°å€æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸')
                    # æ£€æŸ¥ RTSP æµçŠ¶æ€
                    if self.rtsp_cap is None or not self.rtsp_cap.isOpened():
                        self.get_logger().error('âŒ RTSP æµè¿æ¥å·²æ–­å¼€')
                    if not self.rtsp_running:
                        self.get_logger().error('âŒ RTSP è¯»å–çº¿ç¨‹å·²åœæ­¢')
                    return
                
                self._process_with_rtsp_frame(frame)
            else:
                self.get_logger().error(f'âŒ ä¸æ”¯æŒçš„å›¾åƒæºç±»å‹: {image_source}')
                return
            
        except Exception as e:
            self.get_logger().error(f'âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}')
            import traceback
            self.get_logger().error(traceback.format_exc())
        finally:
            with self.processing_lock:
                self.is_processing = False
    
    def ready_callback(self, msg: Bool):
        """
        Ready ä¿¡å·å›è°ƒå‡½æ•°
        msg.data: true è¡¨ç¤ºå‡†å¤‡æ¥å—ç»“æœ
        """
        if msg.data:
            with self.ready_lock:
                if not self.ready_received:
                    self.ready_received = True
                    self.get_logger().info('âœ… æ”¶åˆ° ready ä¿¡å·: å‡†å¤‡æ¥å—ç»“æœ')
                    
                    # å¦‚æœæœ‰ç¼“å­˜çš„ç»“æœï¼Œç«‹å³å‘é€
                    if self.cached_result is not None:
                        self.get_logger().info('ğŸ“¤ å‘é€ç¼“å­˜çš„ç»“æœ...')
                        cached = self.cached_result
                        self.cached_result = None  # å…ˆæ¸…ç©ºç¼“å­˜ï¼Œé¿å…é‡å¤ä½¿ç”¨
                        self._publish_caption(cached)
                        print("\033[36m" + "â”€" * 80 + "\033[0m")
                    else:
                        # æ²¡æœ‰ç¼“å­˜ç»“æœï¼Œç­‰å¾…åç»­å¤„ç†ç»“æœ
                        self.get_logger().info('â³ å½“å‰æ²¡æœ‰ç¼“å­˜ç»“æœï¼Œç­‰å¾…åç»­å¤„ç†å®Œæˆåç›´æ¥å‘é€')
                else:
                    self.get_logger().debug('ready ä¿¡å·å·²æ¥æ”¶è¿‡ï¼Œå¿½ç•¥é‡å¤ä¿¡å·')
        else:
            self.get_logger().debug('æ”¶åˆ° ready=falseï¼Œå¿½ç•¥')
    
    def _process_with_model(self, image_msg: ROSImage):
        """
        æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¤„ç† ROS2 å›¾åƒæ¶ˆæ¯ï¼Œç„¶åé‡Šæ”¾èµ„æº
        
        Args:
            image_msg: ROS2 Image æ¶ˆæ¯
        """
        # 1. è½¬æ¢å›¾åƒ
        self.get_logger().info('ğŸ”„ è½¬æ¢å›¾åƒ...')
        pil_image = self._ros_image_to_pil(image_msg)
        self._process_image(pil_image)
    
    def _process_with_rtsp_frame(self, frame: np.ndarray):
        """
        æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¤„ç† RTSP å¸§ï¼Œç„¶åé‡Šæ”¾èµ„æº
        
        Args:
            frame: RTSP å¸§ï¼ˆRGB numpy arrayï¼‰
        """
        # 1. è½¬æ¢å›¾åƒ
        self.get_logger().info('ğŸ”„ è½¬æ¢å›¾åƒ...')
        pil_image = Image.fromarray(frame)
        self._process_image(pil_image)
    
    def _process_image(self, pil_image: Image.Image):
        """
        å¤„ç†å›¾åƒï¼ˆé€šç”¨æ–¹æ³•ï¼Œæ”¯æŒ ROS2 å’Œ RTSPï¼‰
        
        Args:
            pil_image: PIL Image å¯¹è±¡
        """
        caption_generator = None
        try:
            # 0. æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ¸…ç©ºç¼“å­˜ï¼Œå› ä¸ºç¼“å­˜æ£€æŸ¥åœ¨ control_callback ä¸­å·²ç»å®Œæˆ
            # å¦‚æœè¿›å…¥è¿™é‡Œï¼Œè¯´æ˜æ²¡æœ‰ç¼“å­˜ç»“æœï¼Œå¯ä»¥å®‰å…¨åœ°è¿›è¡Œæ–°çš„å¤„ç†
            
            # 1.1 æ ¹æ® flip å‚æ•°å†³å®šæ˜¯å¦ç¿»è½¬å›¾åƒ
            flip = self.get_parameter('flip').value
            if flip:
                self.get_logger().info('ğŸ”„ æ­£åœ¨å°†å›¾åƒæ—‹è½¬180åº¦...')
                pil_image = pil_image.rotate(180)
            
            # 2. åŠ è½½æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
            self.get_logger().info('ğŸ”„ æ­£åœ¨åŠ è½½ Florence2 æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰...')
            caption_generator = Florence2Caption(
                model_path=self.get_parameter('model_path').value,
                task_type=self.get_parameter('task_type').value,
                max_new_tokens=self.get_parameter('max_new_tokens').value,
                num_beams=self.get_parameter('num_beams').value,
                do_sample=self.get_parameter('do_sample').value,
                trust_remote_code=self.get_parameter('trust_remote_code').value,
            )
            
            # 2.1 å¦‚æœå¯ç”¨ç¿»è¯‘ï¼ŒåŠ è½½ç¿»è¯‘æ¨¡å‹
            translate_to_chinese = self.get_parameter('translate_to_chinese').value
            if translate_to_chinese:
                translation_model = self.get_parameter('translation_model').value
                translation_model_path = self.get_parameter('translation_model_path').value
                # å¦‚æœè·¯å¾„ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨ Noneï¼ˆè¡¨ç¤ºä½¿ç”¨ HuggingFace æ¨¡å‹ï¼‰
                if translation_model_path == '':
                    translation_model_path = None
                caption_generator.set_translation(
                    enable=True, 
                    model_name=translation_model,
                    model_path=translation_model_path
                )
            
            # 3. ç”Ÿæˆæè¿°
            show_timing = self.get_parameter('show_timing').value
            if show_timing:
                caption, timing_info = caption_generator.generate_caption(
                    pil_image, 
                    return_timing=True
                )
                # è®°å½•æ—¶é—´ç»Ÿè®¡
                self.get_logger().info(
                    f'â±ï¸  å¤„ç†æ—¶é—´: {timing_info["total"]:.3f}s, '
                    f'æ¨ç†: {timing_info["inference"]:.3f}s, '
                    f'FPS: {1.0/timing_info["total"]:.2f}'
                )
            else:
                caption = caption_generator.generate_caption(pil_image)
            
            self.get_logger().info(f'âœ… ç”Ÿæˆæè¿°: {caption}')
            
            # 4. æ£€æŸ¥æ˜¯å¦å·²ç»æ¥æ”¶åˆ° ready ä¿¡å·
            with self.ready_lock:
                if self.ready_received:
                    # å·²ç»æ¥æ”¶åˆ° readyï¼Œç›´æ¥å‘é€ç»“æœ
                    self.get_logger().info('ğŸ“¤ ready ä¿¡å·å·²æ¥æ”¶ï¼Œç›´æ¥å‘é€ç»“æœ')
                    self._publish_caption(caption)
                    # å‘é€åæ¸…ç©ºç¼“å­˜ï¼ˆç¡®ä¿ä¸ä¼šé‡å¤ä½¿ç”¨ï¼‰
                    self.cached_result = None
                else:
                    # å°šæœªæ¥æ”¶åˆ° readyï¼Œç¼“å­˜ç»“æœ
                    self.get_logger().info('â³ å°šæœªæ¥æ”¶åˆ° ready ä¿¡å·ï¼Œç¼“å­˜ç»“æœï¼Œç­‰å¾… ready ä¿¡å·...')
                    self.cached_result = caption
            
        finally:
            # 5. é‡Šæ”¾æ¨¡å‹èµ„æº
            if caption_generator is not None:
                self.get_logger().info('ğŸ”„ æ­£åœ¨é‡Šæ”¾æ¨¡å‹èµ„æº...')
                self._cleanup_model(caption_generator)
                self.get_logger().info('âœ… æ¨¡å‹èµ„æºå·²é‡Šæ”¾')
                print("\033[36m" + "â”€" * 80 + "\033[0m")
    
    def _ros_image_to_pil(self, msg: ROSImage) -> Image.Image:
        """
        å°† ROS2 sensor_msgs/Image è½¬æ¢ä¸º PIL.Image
        
        Args:
            msg: ROS2 Image æ¶ˆæ¯
            
        Returns:
            PIL.Image å¯¹è±¡ï¼ˆRGB æ ¼å¼ï¼‰
        """
        try:
            # è½¬æ¢ä¸º OpenCV æ ¼å¼ (BGR)
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            # è½¬æ¢ä¸º RGB
            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸º PIL.Image
            pil_image = Image.fromarray(cv_image_rgb)
            return pil_image
        except Exception as e:
            self.get_logger().error(f'âŒ å›¾åƒè½¬æ¢å¤±è´¥: {e}')
            raise
    
    def _publish_caption(self, caption: str):
        """
        å‘å¸ƒæè¿°ç»“æœ
        
        Args:
            caption: å›¾åƒæè¿°æ–‡æœ¬
        """
        msg = String()
        msg.data = caption
        self.caption_publisher.publish(msg)
        self.get_logger().debug(f'ğŸ“¤ å·²å‘å¸ƒæè¿°ç»“æœ')
    
    def _cleanup_model(self, caption_generator: Florence2Caption):
        """
        æ¸…ç†æ¨¡å‹èµ„æº
        
        Args:
            caption_generator: Florence2Caption å®ä¾‹
        """
        try:
            # åˆ é™¤æ¨¡å‹å’Œå¤„ç†å™¨
            if hasattr(caption_generator, 'model'):
                del caption_generator.model
            if hasattr(caption_generator, 'processor'):
                del caption_generator.processor
            # åˆ é™¤ç¿»è¯‘æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(caption_generator, 'translator_model'):
                del caption_generator.translator_model
            if hasattr(caption_generator, 'translator_tokenizer'):
                del caption_generator.translator_tokenizer
            
            # æ¸…ç† GPU ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
        except Exception as e:
            self.get_logger().warn(f'âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}')
    
    def destroy_node(self):
        """
        èŠ‚ç‚¹é”€æ¯æ—¶æ¸…ç†èµ„æº
        """
        # åœæ­¢ RTSP æµè¯»å–
        if self.rtsp_running:
            self.rtsp_running = False
            if self.rtsp_thread is not None:
                self.rtsp_thread.join(timeout=2.0)
            if self.rtsp_cap is not None:
                self.rtsp_cap.release()
        
        super().destroy_node()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå’Œ ROS2 æ¨¡å¼ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å‘½ä»¤è¡Œæ¨¡å¼ - ä½¿ç”¨è¯¦ç»†æè¿°æ¨¡å¼
  python florence2_caption_ros2.py --image path/to/image.jpg --model_path /path/to/model --task_type detailed_cap

  # ROS2 æ¨¡å¼
  python florence2_caption_ros2.py --ros2
  æˆ–
  ros2 run florence2_caption florence2_caption_ros2
        """,
    )

    parser.add_argument(
        "--ros2",
        action="store_true",
        help="ä»¥ ROS2 èŠ‚ç‚¹æ¨¡å¼è¿è¡Œï¼ˆéœ€è¦ ROS2 ç¯å¢ƒï¼‰",
    )
    parser.add_argument(
        "--image",
        type=str,
        help="è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆå‘½ä»¤è¡Œæ¨¡å¼å¿…éœ€ï¼‰",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="/home/ubun/xanylabeling_data/models/florence",
        help="æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼Œå¦‚ microsoft/Florence-2-large-ftï¼‰",
    )
    parser.add_argument(
        "--task_type",
        type=str,
        default="more_detailed_cap",
        choices=["caption", "detailed_cap", "more_detailed_cap"],
        help="ä»»åŠ¡ç±»å‹: caption (åŸºç¡€æè¿°), detailed_cap (è¯¦ç»†æè¿°), more_detailed_cap (æ›´è¯¦ç»†æè¿°)",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=1024,
        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=3,
        help="Beam search çš„ beam æ•°é‡ï¼ˆé»˜è®¤: 3ï¼‰",
    )
    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="ä½¿ç”¨é‡‡æ ·ç”Ÿæˆï¼ˆé»˜è®¤: Falseï¼Œä½¿ç”¨ beam searchï¼‰",
    )
    parser.add_argument(
        "--show_timing",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†çš„æ¨ç†æ—¶é—´ç»Ÿè®¡",
    )
    parser.add_argument(
        "--translate_to_chinese",
        action="store_true",
        help="å°†ç”Ÿæˆçš„è‹±æ–‡æè¿°ç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆéœ€è¦é¢å¤–åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼‰",
    )
    parser.add_argument(
        "--translation_model",
        type=str,
        default="Helsinki-NLP/opus-mt-en-zh",
        help="ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼Œé»˜è®¤: Helsinki-NLP/opus-mt-en-zhï¼‰",
    )
    parser.add_argument(
        "--translation_model_path",
        type=str,
        default=None,
        help="ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„è€Œä¸æ˜¯ HuggingFace æ¨¡å‹ï¼‰",
    )

    # ä½¿ç”¨ parse_known_args ä»¥å…¼å®¹ ROS2 çš„ --ros-args / --params-file ç­‰å‚æ•°
    # æœªè¯†åˆ«çš„å‚æ•°å°†ä¿ç•™åœ¨ sys.argv ä¸­ï¼Œä¾› rclpy å¤„ç†
    args, _ = parser.parse_known_args()

    # ROS2 æ¨¡å¼
    if args.ros2:
        try:
            rclpy.init()
            # ä½¿ç”¨è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼ˆä¸é¢„åŠ è½½æ¨¡å‹ï¼‰
            node = Florence2ControlNode()
            rclpy.spin(node)
        except KeyboardInterrupt:
            print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­èŠ‚ç‚¹...")
        except Exception as e:
            print(f"âŒ ROS2 èŠ‚ç‚¹è¿è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if rclpy.ok():
                rclpy.shutdown()
        return

    # å‘½ä»¤è¡Œæ¨¡å¼
    if not args.image:
        parser.error("å‘½ä»¤è¡Œæ¨¡å¼éœ€è¦ --image å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --ros2 è¿›å…¥ ROS2 æ¨¡å¼")

    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        caption_generator = Florence2Caption(
            model_path=args.model_path,
            task_type=args.task_type,
            max_new_tokens=args.max_new_tokens,
            do_sample=args.do_sample,
            num_beams=args.num_beams,
        )
        
        # å¦‚æœå¯ç”¨ç¿»è¯‘ï¼Œè®¾ç½®ç¿»è¯‘åŠŸèƒ½
        if args.translate_to_chinese:
            caption_generator.set_translation(
                enable=True, 
                model_name=args.translation_model,
                model_path=args.translation_model_path
            )

        # ç”Ÿæˆæè¿°
        if args.show_timing:
            caption, timing_info = caption_generator.generate_caption(
                args.image, return_timing=True
            )
        else:
            caption = caption_generator.generate_caption(args.image)
            timing_info = None

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“ å›¾åƒæè¿°:")
        print("=" * 60)
        print(caption)
        print("=" * 60)

        # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
        if args.show_timing and timing_info:
            print("\n" + "=" * 60)
            print("â±ï¸  æ¨ç†æ—¶é—´ç»Ÿè®¡")
            print("=" * 60)
            print(f"æ¨¡å‹åŠ è½½æ—¶é—´:     {caption_generator.load_time:>8.3f} ç§’")
            
            # æ˜¾ç¤ºæ¨¡å‹åŠ è½½åçš„å†…å­˜å ç”¨
            if caption_generator.initial_memory:
                print(f"æ¨¡å‹å†…å­˜å ç”¨:     {caption_generator.initial_memory['rss']:>8.2f} MB")
            if caption_generator.initial_gpu_memory:
                print(f"æ¨¡å‹æ˜¾å­˜å ç”¨:     {caption_generator.initial_gpu_memory['allocated']:>8.2f} MB")
            print("-" * 60)
            print(f"å›¾åƒè¯»å–æ—¶é—´:     {timing_info['image_read']:>8.3f} ç§’")
            print(f"é¢„å¤„ç†æ—¶é—´:       {timing_info['preprocess']:>8.3f} ç§’")
            print(f"æ¨¡å‹æ¨ç†æ—¶é—´:     {timing_info['inference']:>8.3f} ç§’")
            print(f"æ–‡æœ¬è§£ç æ—¶é—´:     {timing_info['decode']:>8.3f} ç§’")
            print(f"åå¤„ç†æ—¶é—´:       {timing_info['postprocess']:>8.3f} ç§’")
            if timing_info.get('translation', 0) > 0:
                print(f"ç¿»è¯‘æ—¶é—´:         {timing_info['translation']:>8.3f} ç§’")
            print("-" * 60)
            print(f"å•æ¬¡æ¨ç†æ€»æ—¶é—´:   {timing_info['total']:>8.3f} ç§’")
            print(f"æ¨ç†é€Ÿåº¦:         {1.0/timing_info['total']:>8.2f} FPS")
            if timing_info['inference'] > 0:
                print(f"ç”Ÿæˆ token æ•°:    {timing_info.get('generated_tokens', 0):>8d} tokens")
                print(f"ç”Ÿæˆé€Ÿåº¦:         {timing_info.get('tokens_per_second', 0):>8.2f} tokens/s")
            
            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨
            print("\n" + "-" * 60)
            print("ğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡")
            print("-" * 60)
            if timing_info.get('memory'):
                mem = timing_info['memory']
                print(f"æ¨ç†å‰å†…å­˜:       {mem['before']['rss']:>8.2f} MB")
                print(f"æ¨ç†åå†…å­˜:       {mem['after']['rss']:>8.2f} MB")
                print(f"å†…å­˜å¢é‡:         {mem['delta']['rss']:>8.2f} MB")
                print(f"å³°å€¼å†…å­˜å¢é‡:     {mem['peak_rss']:>8.2f} MB")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨
            if timing_info.get('gpu_memory'):
                print("\n" + "-" * 60)
                print("ğŸ® æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡")
                print("-" * 60)
                gpu_mem = timing_info['gpu_memory']
                print(f"æ¨ç†å‰å·²åˆ†é…æ˜¾å­˜: {gpu_mem['before']['allocated']:>8.2f} MB")
                print(f"æ¨ç†åå·²åˆ†é…æ˜¾å­˜: {gpu_mem['after']['allocated']:>8.2f} MB")
                print(f"æ˜¾å­˜å¢é‡:         {gpu_mem['delta']['allocated']:>8.2f} MB")
                print(f"å³°å€¼æ˜¾å­˜å¢é‡:     {gpu_mem['peak_allocated']:>8.2f} MB")
                print(f"ä¿ç•™æ˜¾å­˜:         {gpu_mem['after']['reserved']:>8.2f} MB (å·²åˆ†é… + ç¼“å­˜æ± )")
                print(f"ç¼“å­˜æ± å¤§å°:       {gpu_mem['after']['reserved'] - gpu_mem['after']['allocated']:>8.2f} MB")
            
            print("=" * 60)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

