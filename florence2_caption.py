#!/usr/bin/env python3
"""
Florence2 å›¾åƒæè¿°ç”Ÿæˆè„šæœ¬
ç‹¬ç«‹ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼Œä¸ä¾èµ– AnyLabeling GUI
"""

import warnings
import sys
import argparse
import time
from pathlib import Path
from unittest.mock import patch
from typing import Dict, Tuple, Union

warnings.filterwarnings("ignore")

try:
    import torch
    from PIL import Image
    from transformers import AutoModelForCausalLM, AutoProcessor
    from transformers.dynamic_module_utils import get_imports
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow")
    sys.exit(1)

# å°è¯•å¯¼å…¥ psutil ç”¨äºå†…å­˜ç›‘æ§
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


class Florence2Caption:
    """Florence2 å›¾åƒæè¿°ç”Ÿæˆå™¨"""

    # ä»»åŠ¡ç±»å‹æ˜ å°„
    TASK_MAPPING = {
        "caption": "<CAPTION>",
        "detailed_cap": "<DETAILED_CAPTION>",
        "more_detailed_cap": "<MORE_DETAILED_CAPTION>",
    }

    def __init__(
        self,
        model_path: str,
        task_type: str = "caption",
        trust_remote_code: bool = True,
        max_new_tokens: int = 1024,
        do_sample: bool = False,
        num_beams: int = 3,
    ):
        """
        åˆå§‹åŒ– Florence2 æ¨¡å‹

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼‰
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰ "caption", "detailed_cap", "more_detailed_cap"
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: beam search çš„ beam æ•°é‡
        """
        if task_type not in self.TASK_MAPPING:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(self.TASK_MAPPING.keys())}"
            )

        self.task_type = task_type
        self.task_token = self.TASK_MAPPING[task_type]
        self.max_new_tokens = max_new_tokens
        self.do_sample = do_sample
        self.num_beams = num_beams

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = (
            torch.float16 if torch.cuda.is_available() else torch.float32
        )

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ”¢ æ•°æ®ç±»å‹: {self.torch_dtype}")

        # æµ‹é‡æ¨¡å‹åŠ è½½æ—¶é—´
        load_start = time.perf_counter()

        # ä¿®å¤ CPU ä¸Š flash_attn çš„é—®é¢˜
        def fixed_get_imports(filename):
            imports = get_imports(filename)
            if not torch.cuda.is_available() and "flash_attn" in imports:
                imports.remove("flash_attn")
            return imports

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        with patch(
            "transformers.dynamic_module_utils.get_imports", fixed_get_imports
        ):
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                device_map=self.device,
                trust_remote_code=trust_remote_code,
            )
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=trust_remote_code,
            )

        load_end = time.perf_counter()
        self.load_time = load_end - load_start
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {self.load_time:.2f} ç§’)")

        # åˆå§‹åŒ– CUDA eventsï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
        self.use_cuda_events = torch.cuda.is_available()
        if self.use_cuda_events:
            self.start_event = torch.cuda.Event(enable_timing=True)
            self.end_event = torch.cuda.Event(enable_timing=True)
            # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()

        # è®°å½•æ¨¡å‹åŠ è½½åçš„å†…å­˜å’Œæ˜¾å­˜å ç”¨
        self.initial_memory = self._get_memory_usage()
        self.initial_gpu_memory = self._get_gpu_memory_usage()

    def _get_memory_usage(self) -> Dict[str, float]:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
        if PSUTIL_AVAILABLE:
            process = psutil.Process()
            mem_info = process.memory_info()
            return {
                "rss": mem_info.rss / (1024 * 1024),  # MB
                "vms": mem_info.vms / (1024 * 1024),  # MB
            }
        return {"rss": 0.0, "vms": 0.0}

    def _get_gpu_memory_usage(self) -> Union[Dict[str, float], None]:
        """
        è·å–å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰
        
        è¿”å›çš„æ˜¾å­˜ä¿¡æ¯è¯´æ˜ï¼š
        - allocated: å·²åˆ†é…æ˜¾å­˜ï¼ŒPyTorch å®é™…ç”¨äºå­˜å‚¨å¼ é‡æ•°æ®çš„æ˜¾å­˜
        - reserved: ä¿ç•™æ˜¾å­˜ï¼ŒPyTorch ä» CUDA åˆ†é…å™¨ä¿ç•™çš„æ€»æ˜¾å­˜
                   åŒ…æ‹¬å·²åˆ†é…çš„æ˜¾å­˜ + ç¼“å­˜æ± ä¸­çš„æ˜¾å­˜ï¼ˆç”¨äºå¿«é€Ÿåˆ†é…æ–°å¼ é‡ï¼‰
        - max_allocated: å³°å€¼å·²åˆ†é…æ˜¾å­˜ï¼Œè‡ªä¸Šæ¬¡ reset_peak_memory_stats() åçš„æœ€å¤§å€¼
        
        æ³¨æ„ï¼šreserved >= allocatedï¼Œå› ä¸º PyTorch ä¼šä¿ç•™ä¸€äº›æ˜¾å­˜ä½œä¸ºç¼“å­˜
        """
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB
            reserved = torch.cuda.memory_reserved() / (1024 * 1024)  # MB
            max_allocated = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB
            return {
                "allocated": allocated,
                "reserved": reserved,
                "max_allocated": max_allocated,
            }
        return None

    def _format_bytes(self, bytes_value: float) -> str:
        """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»æ ¼å¼"""
        if bytes_value < 1024:
            return f"{bytes_value:.2f} MB"
        elif bytes_value < 1024 * 1024:
            return f"{bytes_value / 1024:.2f} GB"
        else:
            return f"{bytes_value / (1024 * 1024):.2f} TB"

    def generate_caption(
        self, image_path: str, return_timing: bool = False
    ) -> Union[str, Tuple[str, Dict[str, float]]]:
        """
        ä¸ºå›¾åƒç”Ÿæˆæè¿°

        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            return_timing: æ˜¯å¦è¿”å›æ—¶é—´ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¦‚æœ return_timing=False: å›¾åƒæè¿°æ–‡æœ¬
            å¦‚æœ return_timing=True: (å›¾åƒæè¿°æ–‡æœ¬, æ—¶é—´ç»Ÿè®¡å­—å…¸)
        """
        timing_info = {}

        # è¯»å–å›¾åƒ
        if not Path(image_path).exists():
            raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

        read_start = time.perf_counter()
        print(f"ğŸ“· æ­£åœ¨è¯»å–å›¾åƒ: {image_path}")
        image = Image.open(image_path).convert("RGB")
        read_end = time.perf_counter()
        timing_info["image_read"] = read_end - read_start

        # é¢„å¤„ç†
        preprocess_start = time.perf_counter()
        prompt = self.task_token
        print(f"ğŸ”¤ ä½¿ç”¨ä»»åŠ¡ç±»å‹: {self.task_type} ({self.task_token})")

        # è®°å½•æ¨ç†å‰çš„å†…å­˜å’Œæ˜¾å­˜
        memory_before = self._get_memory_usage()
        gpu_memory_before = self._get_gpu_memory_usage()
        
        # é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡
        if self.use_cuda_events:
            torch.cuda.reset_peak_memory_stats()

        inputs = self.processor(
            text=prompt, images=image, return_tensors="pt"
        )

        # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡å¹¶åŒ¹é…æ¨¡å‹æ•°æ®ç±»å‹
        model_dtype = next(self.model.parameters()).dtype
        inputs = {
            k: (
                v.to(device=self.device, dtype=model_dtype)
                if torch.is_floating_point(v)
                else v.to(self.device)
            )
            for k, v in inputs.items()
        }

        # åŒæ­¥ GPUï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if self.use_cuda_events:
            torch.cuda.synchronize()

        preprocess_end = time.perf_counter()
        timing_info["preprocess"] = preprocess_end - preprocess_start

        # ç”Ÿæˆæè¿°
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆæè¿°...")
        
        # ä½¿ç”¨ CUDA events æµ‹é‡ GPU æ¨ç†æ—¶é—´ï¼ˆæ›´å‡†ç¡®ï¼‰
        if self.use_cuda_events:
            self.start_event.record()
        else:
            inference_start = time.perf_counter()

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=self.max_new_tokens,
                do_sample=self.do_sample,
                num_beams=self.num_beams,
            )

        if self.use_cuda_events:
            self.end_event.record()
            torch.cuda.synchronize()
            timing_info["inference"] = (
                self.start_event.elapsed_time(self.end_event) / 1000.0
            )  # è½¬æ¢ä¸ºç§’
        else:
            inference_end = time.perf_counter()
            timing_info["inference"] = inference_end - inference_start

        # ç»Ÿè®¡ç”Ÿæˆçš„ token æ•°é‡
        num_generated_tokens = generated_ids.shape[1] - inputs["input_ids"].shape[1]
        timing_info["generated_tokens"] = num_generated_tokens
        if timing_info["inference"] > 0:
            timing_info["tokens_per_second"] = (
                num_generated_tokens / timing_info["inference"]
            )

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        decode_start = time.perf_counter()
        generated_text = self.processor.batch_decode(
            generated_ids, skip_special_tokens=False
        )[0]
        decode_end = time.perf_counter()
        timing_info["decode"] = decode_end - decode_start

        # åå¤„ç†è·å–æè¿°
        postprocess_start = time.perf_counter()
        results = self.processor.post_process_generation(
            generated_text, task=self.task_token, image_size=image.size
        )

        # æå–æè¿°æ–‡æœ¬
        if self.task_token in results:
            caption = results[self.task_token]
            if isinstance(caption, str):
                final_caption = caption
            elif isinstance(caption, dict) and "caption" in caption:
                final_caption = caption["caption"]
            else:
                final_caption = str(caption)
        else:
            final_caption = generated_text

        postprocess_end = time.perf_counter()
        timing_info["postprocess"] = postprocess_end - postprocess_start

        # è®°å½•æ¨ç†åçš„å†…å­˜å’Œæ˜¾å­˜
        memory_after = self._get_memory_usage()
        gpu_memory_after = self._get_gpu_memory_usage()

        # è®¡ç®—å†…å­˜å’Œæ˜¾å­˜ä½¿ç”¨
        timing_info["memory"] = {
            "before": memory_before,
            "after": memory_after,
            "delta": {
                "rss": memory_after["rss"] - memory_before["rss"],
                "vms": memory_after["vms"] - memory_before["vms"],
            },
            "peak_rss": memory_after["rss"] - self.initial_memory["rss"],
        }

        if gpu_memory_before and gpu_memory_after:
            timing_info["gpu_memory"] = {
                "before": gpu_memory_before,
                "after": gpu_memory_after,
                "delta": {
                    "allocated": gpu_memory_after["allocated"]
                    - gpu_memory_before["allocated"],
                    "reserved": gpu_memory_after["reserved"]
                    - gpu_memory_before["reserved"],
                },
                "peak_allocated": gpu_memory_after["max_allocated"]
                - self.initial_gpu_memory["allocated"]
                if self.initial_gpu_memory
                else gpu_memory_after["max_allocated"],
            }
        else:
            timing_info["gpu_memory"] = None

        # è®¡ç®—æ€»æ—¶é—´
        timing_info["total"] = sum(
            [
                timing_info["image_read"],
                timing_info["preprocess"],
                timing_info["inference"],
                timing_info["decode"],
                timing_info["postprocess"],
            ]
        )

        if return_timing:
            return final_caption, timing_info
        else:
            return final_caption

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "processor"):
            del self.processor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python florence2_caption.py --image path/to/image.jpg --model_path microsoft/Florence-2-large-ft

  # ä½¿ç”¨è¯¦ç»†æè¿°æ¨¡å¼
  python florence2_caption.py --image path/to/image.jpg --model_path /path/to/model --task_type detailed_cap

  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
  python florence2_caption.py --image path/to/image.jpg --model_path /home/user/models/florence
        """,
    )

    parser.add_argument(
        "--image",
        type=str,
        required=True,
        help="è¾“å…¥å›¾åƒè·¯å¾„",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="/home/ubun/xanylabeling_data/models/florence",
        help="æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼Œå¦‚ microsoft/Florence-2-large-ftï¼‰",
    )
    parser.add_argument(
        "--task_type",
        type=str,
        default="caption",
        choices=["caption", "detailed_cap", "more_detailed_cap"],
        help="ä»»åŠ¡ç±»å‹: caption (åŸºç¡€æè¿°), detailed_cap (è¯¦ç»†æè¿°), more_detailed_cap (æ›´è¯¦ç»†æè¿°)",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=1024,
        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=3,
        help="Beam search çš„ beam æ•°é‡ï¼ˆé»˜è®¤: 3ï¼‰",
    )
    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="ä½¿ç”¨é‡‡æ ·ç”Ÿæˆï¼ˆé»˜è®¤: Falseï¼Œä½¿ç”¨ beam searchï¼‰",
    )
    parser.add_argument(
        "--show_timing",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†çš„æ¨ç†æ—¶é—´ç»Ÿè®¡",
    )

    args = parser.parse_args()

    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model_init_start = time.perf_counter()
        caption_generator = Florence2Caption(
            model_path=args.model_path,
            task_type=args.task_type,
            max_new_tokens=args.max_new_tokens,
            do_sample=args.do_sample,
            num_beams=args.num_beams,
        )
        model_init_end = time.perf_counter()
        model_init_time = model_init_end - model_init_start

        # ç”Ÿæˆæè¿°
        if args.show_timing:
            caption, timing_info = caption_generator.generate_caption(
                args.image, return_timing=True
            )
        else:
            caption = caption_generator.generate_caption(args.image)
            timing_info = None

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“ å›¾åƒæè¿°:")
        print("=" * 60)
        print(caption)
        print("=" * 60)

        # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
        if args.show_timing and timing_info:
            print("\n" + "=" * 60)
            print("â±ï¸  æ¨ç†æ—¶é—´ç»Ÿè®¡")
            print("=" * 60)
            print(f"æ¨¡å‹åŠ è½½æ—¶é—´:     {caption_generator.load_time:>8.3f} ç§’")
            
            # æ˜¾ç¤ºæ¨¡å‹åŠ è½½åçš„å†…å­˜å ç”¨
            if caption_generator.initial_memory:
                print(f"æ¨¡å‹å†…å­˜å ç”¨:     {caption_generator.initial_memory['rss']:>8.2f} MB")
            if caption_generator.initial_gpu_memory:
                print(f"æ¨¡å‹æ˜¾å­˜å ç”¨:     {caption_generator.initial_gpu_memory['allocated']:>8.2f} MB")
            print("-" * 60)
            print(f"å›¾åƒè¯»å–æ—¶é—´:     {timing_info['image_read']:>8.3f} ç§’")
            print(f"é¢„å¤„ç†æ—¶é—´:       {timing_info['preprocess']:>8.3f} ç§’")
            print(f"æ¨¡å‹æ¨ç†æ—¶é—´:     {timing_info['inference']:>8.3f} ç§’")
            print(f"æ–‡æœ¬è§£ç æ—¶é—´:     {timing_info['decode']:>8.3f} ç§’")
            print(f"åå¤„ç†æ—¶é—´:       {timing_info['postprocess']:>8.3f} ç§’")
            print("-" * 60)
            print(f"å•æ¬¡æ¨ç†æ€»æ—¶é—´:   {timing_info['total']:>8.3f} ç§’")
            print(f"æ¨ç†é€Ÿåº¦:         {1.0/timing_info['total']:>8.2f} FPS")
            if timing_info['inference'] > 0:
                print(f"ç”Ÿæˆ token æ•°:    {timing_info.get('generated_tokens', 0):>8d} tokens")
                print(f"ç”Ÿæˆé€Ÿåº¦:         {timing_info.get('tokens_per_second', 0):>8.2f} tokens/s")
            
            # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨
            print("\n" + "-" * 60)
            print("ğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡")
            print("-" * 60)
            if timing_info.get('memory'):
                mem = timing_info['memory']
                print(f"æ¨ç†å‰å†…å­˜:       {mem['before']['rss']:>8.2f} MB")
                print(f"æ¨ç†åå†…å­˜:       {mem['after']['rss']:>8.2f} MB")
                print(f"å†…å­˜å¢é‡:         {mem['delta']['rss']:>8.2f} MB")
                print(f"å³°å€¼å†…å­˜å¢é‡:     {mem['peak_rss']:>8.2f} MB")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨
            if timing_info.get('gpu_memory'):
                print("\n" + "-" * 60)
                print("ğŸ® æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡")
                print("-" * 60)
                gpu_mem = timing_info['gpu_memory']
                print(f"æ¨ç†å‰å·²åˆ†é…æ˜¾å­˜: {gpu_mem['before']['allocated']:>8.2f} MB")
                print(f"æ¨ç†åå·²åˆ†é…æ˜¾å­˜: {gpu_mem['after']['allocated']:>8.2f} MB")
                print(f"æ˜¾å­˜å¢é‡:         {gpu_mem['delta']['allocated']:>8.2f} MB")
                print(f"å³°å€¼æ˜¾å­˜å¢é‡:     {gpu_mem['peak_allocated']:>8.2f} MB")
                print(f"ä¿ç•™æ˜¾å­˜:         {gpu_mem['after']['reserved']:>8.2f} MB (å·²åˆ†é… + ç¼“å­˜æ± )")
                print(f"ç¼“å­˜æ± å¤§å°:       {gpu_mem['after']['reserved'] - gpu_mem['after']['allocated']:>8.2f} MB")
            
            print("=" * 60)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

