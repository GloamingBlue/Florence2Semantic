#!/usr/bin/env python3
"""
Florence2 å›¾åƒæè¿°ç”Ÿæˆè„šæœ¬ï¼ˆç²¾ç®€ç‰ˆï¼‰
ç‹¬ç«‹ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼Œä¸ä¾èµ– AnyLabeling GUI
ç²¾ç®€ç‰ˆï¼šç§»é™¤äº†æ€§èƒ½ç›‘æµ‹å’Œæ—¶é—´è®¡ç®—åŠŸèƒ½ï¼Œåªä¿ç•™æ ¸å¿ƒè¯­ä¹‰ç”ŸæˆåŠŸèƒ½
"""

import warnings
import sys
import argparse
import threading
import gc
from pathlib import Path
from unittest.mock import patch
from typing import Union, Optional

warnings.filterwarnings("ignore")

# ç¿»è¯‘ç›¸å…³å¯¼å…¥
from transformers import MarianMTModel, MarianTokenizer

try:
    import torch
    from PIL import Image
    import numpy as np
    from transformers import AutoModelForCausalLM, AutoProcessor
    from transformers.dynamic_module_utils import get_imports
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install torch transformers pillow numpy")
    sys.exit(1)

# å¯¼å…¥ ROS2 ç›¸å…³åº“
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image as ROSImage
from std_msgs.msg import Int8, String

# å¯¼å…¥ cv_bridge å’Œ OpenCVï¼Œç”¨äºå›¾åƒè½¬æ¢
from cv_bridge import CvBridge
import cv2


class Florence2Caption:
    """Florence2 å›¾åƒæè¿°ç”Ÿæˆå™¨ï¼ˆç²¾ç®€ç‰ˆï¼‰"""

    # ä»»åŠ¡ç±»å‹æ˜ å°„
    TASK_MAPPING = {
        "caption": "<CAPTION>",
        "detailed_cap": "<DETAILED_CAPTION>",
        "more_detailed_cap": "<MORE_DETAILED_CAPTION>",
    }

    def __init__(
        self,
        model_path: str,
        task_type: str = "caption",
        trust_remote_code: bool = True,
        max_new_tokens: int = 1024,
        do_sample: bool = False,
        num_beams: int = 3,
    ):
        """
        åˆå§‹åŒ– Florence2 æ¨¡å‹

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼‰
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰ "caption", "detailed_cap", "more_detailed_cap"
            trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: beam search çš„ beam æ•°é‡
        """
        if task_type not in self.TASK_MAPPING:
            raise ValueError(
                f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}ã€‚"
                f"æ”¯æŒçš„ç±»å‹: {list(self.TASK_MAPPING.keys())}"
            )

        self.task_type = task_type
        self.task_token = self.TASK_MAPPING[task_type]
        self.max_new_tokens = max_new_tokens
        self.do_sample = do_sample
        self.num_beams = num_beams

        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = (
            torch.float16 if torch.cuda.is_available() else torch.float32
        )

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ä¿®å¤ CPU ä¸Š flash_attn çš„é—®é¢˜
        def fixed_get_imports(filename):
            imports = get_imports(filename)
            if not torch.cuda.is_available() and "flash_attn" in imports:
                imports.remove("flash_attn")
            return imports

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        with patch(
            "transformers.dynamic_module_utils.get_imports", fixed_get_imports
        ):
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=self.torch_dtype,
                device_map=self.device,
                trust_remote_code=trust_remote_code,
            )
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=trust_remote_code,
            )

        print(f"âœ… Caption æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç¿»è¯‘æ¨¡å‹ï¼ˆå¯é€‰ï¼ŒæŒ‰éœ€åŠ è½½ï¼‰
        self.translator = None
        self.translate_to_chinese = False

    def set_translation(
        self, 
        enable: bool = True, 
        model_name: str = "Helsinki-NLP/opus-mt-en-zh",
        model_path: Optional[str] = None
    ):
        """
        è®¾ç½®æ˜¯å¦å¯ç”¨ç¿»è¯‘åŠŸèƒ½
        
        Args:
            enable: æ˜¯å¦å¯ç”¨ç¿»è¯‘
            model_name: ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼‰ï¼Œå½“ model_path ä¸º None æ—¶ä½¿ç”¨
                      - "Helsinki-NLP/opus-mt-en-zh" (æ¨èï¼Œè‹±æ–‡åˆ°ä¸­æ–‡)
                      - "facebook/nllb-200-distilled-600M" (å¤šè¯­è¨€ï¼Œéœ€è¦æŒ‡å®šè¯­è¨€ä»£ç )
            model_path: æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼‰
        """
        self.translate_to_chinese = enable
        
        if enable and self.translator is None:
            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
            if model_path and Path(model_path).exists():
                print(f"ğŸ”„ æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_path}")
                try:
                    self.translator_tokenizer = MarianTokenizer.from_pretrained(model_path)
                    self.translator_model = MarianMTModel.from_pretrained(model_path)
                    if torch.cuda.is_available():
                        self.translator_model = self.translator_model.to(self.device)
                    print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆæœ¬åœ°è·¯å¾„ï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ä»æœ¬åœ°è·¯å¾„åŠ è½½ç¿»è¯‘æ¨¡å‹å¤±è´¥: {e}")
                    self.translate_to_chinese = False
            elif model_path:
                print(f"âš ï¸  æœ¬åœ°ç¿»è¯‘æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}ï¼Œå°†ä½¿ç”¨ HuggingFace æ¨¡å‹")
                # å›é€€åˆ° HuggingFace æ¨¡å‹
                model_path = None
            
            if enable and model_path is None:
                print(f"ğŸ”„ æ­£åœ¨ä» HuggingFace åŠ è½½ç¿»è¯‘æ¨¡å‹: {model_name}")
                try:
                    self.translator_tokenizer = MarianTokenizer.from_pretrained(model_name)
                    self.translator_model = MarianMTModel.from_pretrained(model_name)
                    if torch.cuda.is_available():
                        self.translator_model = self.translator_model.to(self.device)
                    print(f"âœ… ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæˆï¼ˆHuggingFaceï¼‰")
                except Exception as e:
                    print(f"âš ï¸  ç¿»è¯‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.translate_to_chinese = False

    def _translate_to_chinese(self, text: str) -> str:
        """
        å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡
        
        Args:
            text: è‹±æ–‡æ–‡æœ¬
            
        Returns:
            ä¸­æ–‡æ–‡æœ¬
        """
        if not self.translate_to_chinese or self.translator_model is None:
            return text
        
        try:
            # ç¿»è¯‘
            inputs = self.translator_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                translated = self.translator_model.generate(**inputs, max_length=512)
            
            translated_text = self.translator_tokenizer.decode(translated[0], skip_special_tokens=True)
            return translated_text
        except Exception as e:
            print(f"âš ï¸  ç¿»è¯‘å¤±è´¥: {e}ï¼Œè¿”å›åŸæ–‡")
            return text

    def generate_caption(
        self, 
        image: Union[str, Image.Image, np.ndarray]
    ) -> str:
        """
        ä¸ºå›¾åƒç”Ÿæˆæè¿°

        Args:
            image: å›¾åƒè¾“å…¥ï¼Œå¯ä»¥æ˜¯ï¼š
                  - str: å›¾åƒæ–‡ä»¶è·¯å¾„
                  - PIL.Image: PIL å›¾åƒå¯¹è±¡
                  - np.ndarray: numpy æ•°ç»„ï¼ˆRGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼‰

        Returns:
            å›¾åƒæè¿°æ–‡æœ¬
        """
        # è¯»å–å’Œè½¬æ¢å›¾åƒ
        if isinstance(image, str):
            # æ–‡ä»¶è·¯å¾„
            if not Path(image).exists():
                raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image}")
            pil_image = Image.open(image).convert("RGB")
        elif isinstance(image, Image.Image):
            # PIL.Image
            pil_image = image.convert("RGB")
        elif isinstance(image, np.ndarray):
            # numpy æ•°ç»„
            if len(image.shape) != 3 or image.shape[2] != 3:
                raise ValueError(f"numpy æ•°ç»„å¿…é¡»æ˜¯ RGB æ ¼å¼ï¼Œshape: [H, W, 3]ï¼Œå½“å‰: {image.shape}")
            pil_image = Image.fromarray(image.astype(np.uint8))
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}ï¼Œæ”¯æŒç±»å‹: str, PIL.Image, np.ndarray")

        # é¢„å¤„ç†
        prompt = self.task_token
        inputs = self.processor(
            text=prompt, images=pil_image, return_tensors="pt"
        )

        # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡å¹¶åŒ¹é…æ¨¡å‹æ•°æ®ç±»å‹
        model_dtype = next(self.model.parameters()).dtype
        inputs = {
            k: (
                v.to(device=self.device, dtype=model_dtype)
                if torch.is_floating_point(v)
                else v.to(self.device)
            )
            for k, v in inputs.items()
        }

        # ç”Ÿæˆæè¿°
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆæè¿°...")
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=self.max_new_tokens,
                do_sample=self.do_sample,
                num_beams=self.num_beams,
            )

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.processor.batch_decode(
            generated_ids, skip_special_tokens=False
        )[0]

        # åå¤„ç†è·å–æè¿°
        results = self.processor.post_process_generation(
            generated_text, task=self.task_token, image_size=pil_image.size
        )

        # æå–æè¿°æ–‡æœ¬
        if self.task_token in results:
            caption = results[self.task_token]
            if isinstance(caption, str):
                final_caption = caption
            elif isinstance(caption, dict) and "caption" in caption:
                final_caption = caption["caption"]
            else:
                final_caption = str(caption)
        else:
            final_caption = generated_text

        # å¦‚æœå¯ç”¨äº†ç¿»è¯‘ï¼Œå°†è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡
        if self.translate_to_chinese:
            print("ğŸ”„ æ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
            final_caption = self._translate_to_chinese(final_caption)

        return final_caption

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, "model"):
            del self.model
        if hasattr(self, "processor"):
            del self.processor
        if hasattr(self, "translator_model"):
            del self.translator_model
        if hasattr(self, "translator_tokenizer"):
            del self.translator_tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# ROS2 è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰
class Florence2ControlNode(Node):
    """
    è½»é‡çº§æ§åˆ¶èŠ‚ç‚¹ï¼Œè´Ÿè´£ï¼š
    - æŒç»­æ¥æ”¶å›¾åƒæµï¼Œä¿å­˜æœ€æ–°ä¸€å¸§
    - ç›‘å¬æ§åˆ¶ä¿¡å·
    - æ”¶åˆ°ä¿¡å· 1 æ—¶ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹ã€å¤„ç†å›¾åƒã€é‡Šæ”¾èµ„æº
    """
    
    def __init__(self):
        super().__init__('florence2_control_node')
        
        # å‚æ•°å£°æ˜
        self.declare_parameter('image_topic', '/camera/camera/color/image_raw')
        self.declare_parameter('control_topic', '/navigation/florence')
        self.declare_parameter('model_path', '/home/ubun/xanylabeling_data/models/florence')
        self.declare_parameter('task_type', 'more_detailed_cap')
        self.declare_parameter('result_topic', '/florence2/caption')
        self.declare_parameter('max_new_tokens', 1024)
        self.declare_parameter('num_beams', 3)
        self.declare_parameter('do_sample', False)
        self.declare_parameter('trust_remote_code', True)
        self.declare_parameter('translate_to_chinese', True)  # æ˜¯å¦ç¿»è¯‘ä¸ºä¸­æ–‡
        self.declare_parameter('translation_model', 'Helsinki-NLP/opus-mt-en-zh')  # ç¿»è¯‘æ¨¡å‹ï¼ˆHuggingFace IDï¼‰
        self.declare_parameter('translation_model_path', '')  # ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        self.declare_parameter('flip', False)  # æ˜¯å¦åœ¨è¯­ä¹‰ç”Ÿæˆå‰å°†å›¾åƒæ—‹è½¬180åº¦
        
        # çº¿ç¨‹å®‰å…¨ï¼šæœ€æ–°å›¾åƒå­˜å‚¨
        self.latest_image_lock = threading.Lock()
        self.latest_image_msg = None
        
        # å¤„ç†çŠ¶æ€æ ‡å¿—ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        self.is_processing = False
        self.processing_lock = threading.Lock()
        
        # åˆå§‹åŒ– cv_bridgeï¼ˆç”¨äºå›¾åƒè½¬æ¢ï¼‰
        self.cv_bridge = CvBridge()
        
        # æ³¨æ„ï¼šæ—¥å¿—æ ¼å¼åŒ–å™¨å·²åœ¨ rclpy.init() åå…¨å±€è®¾ç½®ï¼Œè¿™é‡Œä¸éœ€è¦å†è®¾ç½®
        
        # åˆ›å»ºå›¾åƒè®¢é˜…è€…ï¼ˆæŒç»­æ¥æ”¶ï¼Œä¿å­˜æœ€æ–°å¸§ï¼‰
        image_topic = self.get_parameter('image_topic').value
        self.image_subscription = self.create_subscription(
            ROSImage,
            image_topic,
            self.image_callback,
            1  # QoS depth = 1ï¼Œåªä¿ç•™æœ€æ–°å›¾åƒ
        )
        self.get_logger().info(f'ğŸ“· å·²è®¢é˜…å›¾åƒè¯é¢˜: {image_topic}')
        
        # åˆ›å»ºæ§åˆ¶ä¿¡å·è®¢é˜…è€…
        control_topic = self.get_parameter('control_topic').value
        self.control_subscription = self.create_subscription(
            Int8,
            control_topic,
            self.control_callback,
            10  # QoS depth = 10ï¼Œç¡®ä¿ä¿¡å·ä¸ä¸¢å¤±
        )
        self.get_logger().info(f'ğŸ® å·²è®¢é˜…æ§åˆ¶ä¿¡å·è¯é¢˜: {control_topic}')
        
        # åˆ›å»ºç»“æœå‘å¸ƒè€…
        result_topic = self.get_parameter('result_topic').value
        self.caption_publisher = self.create_publisher(
            String,
            result_topic,
            10
        )
        self.get_logger().info(f'ğŸ“¤ å·²åˆ›å»ºç»“æœå‘å¸ƒè¯é¢˜: {result_topic}')
        
        self.get_logger().info('âœ… Florence2 Control Node åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰')
        self.get_logger().info('â³ ç­‰å¾…æ§åˆ¶ä¿¡å·...')
    
    def image_callback(self, msg: ROSImage):
        """
        å›¾åƒè¯é¢˜å›è°ƒå‡½æ•° - æŒç»­æ¥æ”¶ï¼Œä¿å­˜æœ€æ–°ä¸€å¸§
        """
        with self.latest_image_lock:
            self.latest_image_msg = msg
    
    def control_callback(self, msg: Int8):
        """
        æ§åˆ¶ä¿¡å·å›è°ƒå‡½æ•°
        msg.data: 0 = ä¸å¤„ç†, 1 = å¤„ç†
        """
        signal = msg.data
        
        if signal == 0:
            # æ”¶åˆ° 0ï¼Œä¸å¤„ç†
            self.get_logger().debug('æ”¶åˆ°æ§åˆ¶ä¿¡å· 0: è·³è¿‡å¤„ç†')
            return
        
        if signal == 1:
            # æ”¶åˆ° 1ï¼ŒæŒ‰éœ€åŠ è½½æ¨¡å‹å¹¶å¤„ç†
            self.get_logger().info('æ”¶åˆ°æ§åˆ¶ä¿¡å· 1: å¼€å§‹å¤„ç†å›¾åƒ...')
            
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            with self.processing_lock:
                if self.is_processing:
                    self.get_logger().warn('ä¸Šä¸€æ¬¡å¤„ç†å°šæœªå®Œæˆï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚')
                    return
                self.is_processing = True
            
            try:
                # è·å–æœ€æ–°å›¾åƒ
                with self.latest_image_lock:
                    if self.latest_image_msg is None:
                        self.get_logger().warn('âš ï¸  å°šæœªæ”¶åˆ°å›¾åƒï¼Œæ— æ³•å¤„ç†')
                        return
                    image_msg = self.latest_image_msg
                
                # æŒ‰éœ€åŠ è½½æ¨¡å‹å¹¶å¤„ç†
                self._process_with_model(image_msg)
                
            except Exception as e:
                self.get_logger().error(f'âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}')
                import traceback
                self.get_logger().error(traceback.format_exc())
            finally:
                with self.processing_lock:
                    self.is_processing = False
        else:
            self.get_logger().warn(f'âš ï¸  æ”¶åˆ°æœªçŸ¥æ§åˆ¶ä¿¡å·: {signal}ï¼ŒæœŸæœ› 0 æˆ– 1')
    
    def _process_with_model(self, image_msg: ROSImage):
        """
        æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œå¤„ç†å›¾åƒï¼Œç„¶åé‡Šæ”¾èµ„æº
        
        Args:
            image_msg: ROS2 Image æ¶ˆæ¯
        """
        caption_generator = None
        try:
            # 1. è½¬æ¢å›¾åƒ
            self.get_logger().info('ğŸ”„ è½¬æ¢å›¾åƒ...')
            pil_image = self._ros_image_to_pil(image_msg)
            
            # 1.1 æ ¹æ® flip å‚æ•°å†³å®šæ˜¯å¦ç¿»è½¬å›¾åƒ
            flip = self.get_parameter('flip').value
            if flip:
                self.get_logger().info('ğŸ”„ æ­£åœ¨å°†å›¾åƒæ—‹è½¬180åº¦...')
                pil_image = pil_image.rotate(180)
            
            # 2. åŠ è½½æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
            self.get_logger().info('ğŸ”„ æ­£åœ¨åŠ è½½ Florence2 æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰...')
            caption_generator = Florence2Caption(
                model_path=self.get_parameter('model_path').value,
                task_type=self.get_parameter('task_type').value,
                max_new_tokens=self.get_parameter('max_new_tokens').value,
                num_beams=self.get_parameter('num_beams').value,
                do_sample=self.get_parameter('do_sample').value,
                trust_remote_code=self.get_parameter('trust_remote_code').value,
            )
            
            # 2.1 å¦‚æœå¯ç”¨ç¿»è¯‘ï¼ŒåŠ è½½ç¿»è¯‘æ¨¡å‹
            translate_to_chinese = self.get_parameter('translate_to_chinese').value
            if translate_to_chinese:
                translation_model = self.get_parameter('translation_model').value
                translation_model_path = self.get_parameter('translation_model_path').value
                # å¦‚æœè·¯å¾„ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨ Noneï¼ˆè¡¨ç¤ºä½¿ç”¨ HuggingFace æ¨¡å‹ï¼‰
                if translation_model_path == '':
                    translation_model_path = None
                caption_generator.set_translation(
                    enable=True, 
                    model_name=translation_model,
                    model_path=translation_model_path
                )
            
            # 3. ç”Ÿæˆæè¿°
            caption = caption_generator.generate_caption(pil_image)
            
            # 4. å‘å¸ƒç»“æœ
            self._publish_caption(caption)
            
            self.get_logger().info(f'âœ… ç”Ÿæˆæè¿°: {caption}')
            
        finally:
            # 5. é‡Šæ”¾æ¨¡å‹èµ„æº
            if caption_generator is not None:
                self.get_logger().info('ğŸ”„ æ­£åœ¨é‡Šæ”¾æ¨¡å‹èµ„æº...')
                self._cleanup_model(caption_generator)
                self.get_logger().info('âœ… æ¨¡å‹èµ„æºå·²é‡Šæ”¾')
                print("\033[36m" + "â”€" * 80 + "\033[0m")
    
    def _ros_image_to_pil(self, msg: ROSImage) -> Image.Image:
        """
        å°† ROS2 sensor_msgs/Image è½¬æ¢ä¸º PIL.Image
        
        Args:
            msg: ROS2 Image æ¶ˆæ¯
            
        Returns:
            PIL.Image å¯¹è±¡ï¼ˆRGB æ ¼å¼ï¼‰
        """
        try:
            # è½¬æ¢ä¸º OpenCV æ ¼å¼ (BGR)
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            # è½¬æ¢ä¸º RGB
            cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸º PIL.Image
            pil_image = Image.fromarray(cv_image_rgb)
            return pil_image
        except Exception as e:
            self.get_logger().error(f'âŒ å›¾åƒè½¬æ¢å¤±è´¥: {e}')
            raise
    
    def _publish_caption(self, caption: str):
        """
        å‘å¸ƒæè¿°ç»“æœ
        
        Args:
            caption: å›¾åƒæè¿°æ–‡æœ¬
        """
        msg = String()
        msg.data = caption
        self.caption_publisher.publish(msg)
        self.get_logger().debug(f'ğŸ“¤ å·²å‘å¸ƒæè¿°ç»“æœ')
    
    def _cleanup_model(self, caption_generator: Florence2Caption):
        """
        æ¸…ç†æ¨¡å‹èµ„æº
        
        Args:
            caption_generator: Florence2Caption å®ä¾‹
        """
        try:
            # åˆ é™¤æ¨¡å‹å’Œå¤„ç†å™¨
            if hasattr(caption_generator, 'model'):
                del caption_generator.model
            if hasattr(caption_generator, 'processor'):
                del caption_generator.processor
            # åˆ é™¤ç¿»è¯‘æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(caption_generator, 'translator_model'):
                del caption_generator.translator_model
            if hasattr(caption_generator, 'translator_tokenizer'):
                del caption_generator.translator_tokenizer
            
            # æ¸…ç† GPU ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception as e:
            self.get_logger().warn(f'âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}')


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Florence2 æ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ï¼ˆç²¾ç®€ç‰ˆï¼Œæ”¯æŒå‘½ä»¤è¡Œå’Œ ROS2 æ¨¡å¼ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å‘½ä»¤è¡Œæ¨¡å¼
  python florence2_caption_ros2_lite.py --image path/to/image.jpg --model_path /path/to/model --task_type detailed_cap

  # ROS2 æ¨¡å¼
  python florence2_caption_ros2_lite.py --ros2
        """,
    )

    parser.add_argument(
        "--ros2",
        action="store_true",
        help="ä»¥ ROS2 èŠ‚ç‚¹æ¨¡å¼è¿è¡Œï¼ˆéœ€è¦ ROS2 ç¯å¢ƒï¼‰",
    )
    parser.add_argument(
        "--image",
        type=str,
        help="è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆå‘½ä»¤è¡Œæ¨¡å¼å¿…éœ€ï¼‰",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="/home/ubun/xanylabeling_data/models/florence",
        help="æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace æ¨¡å‹ IDï¼Œå¦‚ microsoft/Florence-2-large-ftï¼‰",
    )
    parser.add_argument(
        "--task_type",
        type=str,
        default="more_detailed_cap",
        choices=["caption", "detailed_cap", "more_detailed_cap"],
        help="ä»»åŠ¡ç±»å‹: caption (åŸºç¡€æè¿°), detailed_cap (è¯¦ç»†æè¿°), more_detailed_cap (æ›´è¯¦ç»†æè¿°)",
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=1024,
        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 1024ï¼‰",
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=3,
        help="Beam search çš„ beam æ•°é‡ï¼ˆé»˜è®¤: 3ï¼‰",
    )
    parser.add_argument(
        "--do_sample",
        action="store_true",
        help="ä½¿ç”¨é‡‡æ ·ç”Ÿæˆï¼ˆé»˜è®¤: Falseï¼Œä½¿ç”¨ beam searchï¼‰",
    )
    parser.add_argument(
        "--translate_to_chinese",
        action="store_true",
        help="å°†ç”Ÿæˆçš„è‹±æ–‡æè¿°ç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆéœ€è¦é¢å¤–åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼‰",
    )
    parser.add_argument(
        "--translation_model",
        type=str,
        default="Helsinki-NLP/opus-mt-en-zh",
        help="ç¿»è¯‘æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼Œé»˜è®¤: Helsinki-NLP/opus-mt-en-zhï¼‰",
    )
    parser.add_argument(
        "--translation_model_path",
        type=str,
        default=None,
        help="ç¿»è¯‘æ¨¡å‹æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„è€Œä¸æ˜¯ HuggingFace æ¨¡å‹ï¼‰",
    )

    # ä½¿ç”¨ parse_known_args ä»¥å…¼å®¹ ROS2 çš„ --ros-args / --params-file ç­‰å‚æ•°
    args, _ = parser.parse_known_args()

    # ROS2 æ¨¡å¼
    if args.ros2:
        try:
            rclpy.init()
            node = Florence2ControlNode()
            rclpy.spin(node)
        except KeyboardInterrupt:
            print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­èŠ‚ç‚¹...")
        except Exception as e:
            print(f"âŒ ROS2 èŠ‚ç‚¹è¿è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if rclpy.ok():
                rclpy.shutdown()
        return

    # å‘½ä»¤è¡Œæ¨¡å¼
    if not args.image:
        parser.error("å‘½ä»¤è¡Œæ¨¡å¼éœ€è¦ --image å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --ros2 è¿›å…¥ ROS2 æ¨¡å¼")

    try:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        caption_generator = Florence2Caption(
            model_path=args.model_path,
            task_type=args.task_type,
            max_new_tokens=args.max_new_tokens,
            do_sample=args.do_sample,
            num_beams=args.num_beams,
        )
        
        # å¦‚æœå¯ç”¨ç¿»è¯‘ï¼Œè®¾ç½®ç¿»è¯‘åŠŸèƒ½
        if args.translate_to_chinese:
            caption_generator.set_translation(
                enable=True, 
                model_name=args.translation_model,
                model_path=args.translation_model_path
            )

        # ç”Ÿæˆæè¿°
        caption = caption_generator.generate_caption(args.image)

        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“ å›¾åƒæè¿°:")
        print("=" * 60)
        print(caption)
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

